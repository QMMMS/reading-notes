# LangChain

> [ref](https://qmmms.github.io/posts/LangChain%E5%AE%9E%E6%88%98/)

è¯·è®¾æƒ³ä¸‹é¢è¿™å‡ ç§åœºæ™¯ï¼šä½ æ‹¥æœ‰å‡ æœ¬ç”µå­ä¹¦ï¼Œæˆ–å‡ åä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ŒæŠ‘æˆ–æ˜¯åˆ©ç”¨æ•°æ® åº“å®Œæˆç‰¹å®šä»»åŠ¡,æˆ‘ä»¬æƒ³è¦LLMsæ¨¡å‹å­¦ä¹ ç”¨æˆ·ç»™å®šçš„æ•°æ®ï¼Œå¹¶ä¸”åªå›ç­”ç»™å®šæ•°æ®èŒƒå›´ å†…çš„ç›¸å…³é—®é¢˜,å¦‚æœé—®é¢˜è¶…å‡ºèŒƒå›´,ä¸€å¾‹å‘ŠçŸ¥ç”¨æˆ·é—®é¢˜è¶…å‡ºèŒƒå›´æ— æ³•å›ç­”,ä¹Ÿå°±æ˜¯æˆ‘ä»¬ è¦é™åˆ¶LLMsæ¨¡å‹è‡ªç”±å‘æŒ¥ï¼Œä¸èƒ½è®©å®ƒéšä¾¿ä¹±è¯´ã€‚å¦‚ä½•åŸºäºå¤§æ¨¡å‹å®Œæˆä¸Šè¿°ä»»åŠ¡ï¼Ÿ LangChainå¯ä»¥å¸®ä½ å®ç°ã€‚ç‚¹

LangChainå…±æœ‰6ä¸ªæ¯”è¾ƒæ ¸å¿ƒçš„æ¨¡å—ï¼Œåˆ†åˆ«æ˜¯ï¼š 

- æ¨¡å‹ï¼ˆModelsï¼‰ï¼šä½¿ç”¨çš„ä¸åŒçš„è¯­è¨€æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ã€‚ 
- æç¤ºï¼ˆPromptsï¼‰ï¼šæ„å»ºæ¨¡å‹çš„è¾“å…¥ã€‚ 
- é“¾ï¼ˆChainsï¼‰ï¼šç»“åˆLLMså’Œå…¶ä»–ç»„ä»¶ã€‚ 
- ç´¢å¼•ï¼ˆIndexesï¼‰ï¼šè®¿é—®å¤–éƒ¨æ•°æ®ã€‚ 
- è®°å¿†ï¼ˆMemoryï¼‰ï¼šå­˜å‚¨ä»¥å¾€çš„äº¤äº’ä¿¡æ¯ã€‚ 
- ä»£ç†ï¼ˆAgentsï¼‰ï¼šè°ƒç”¨å…¶ä»–å·¥å…·ã€‚

![](./img/lc.png)

## 0ã€å‡†å¤‡

åœ¨è¿›è¡Œå®éªŒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®**ä»£ç†**å’Œæ£€æŸ¥ç¯å¢ƒã€‚


```python
import os

os.environ['HTTP_PROXY'] = 'http://127.0.0.1:xxxxx'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:xxxxx'

# æ£€æŸ¥ä»£ç†æ˜¯å¦æœ‰ç”¨
def check_proxy():
    import urllib.request
    url = "https://www.google.com"
    filename = "google.html"
    urllib.request.urlretrieve(url, filename)

check_proxy()  # å¦‚æœå¯ä»¥æ­£å¸¸ä¿å­˜ google ç½‘é¡µï¼Œä»£ç†è¿è¡Œæ­£å¸¸
```

ä¸‹é¢æˆ‘ä»¬ç»§ç»­æ£€æŸ¥OpenAI API KEYæ˜¯å¦å¯ç”¨ã€‚


```python
import os
from openai import OpenAI

os.environ["OPENAI_API_KEY"] = "xxxxxxxxxxx"
client = OpenAI()

response = client.completions.create(
    model="davinci-002",
    prompt="Hello, my name is",
    max_tokens=10,
    n=1,
    stop=None,
    temperature=0.5,
)

print(response.choices[0].text)
```

     Dr. Mike. I am a board certified psychiatrist


> æ³¨æ„ï¼Œå¦‚æœæç¤ºæ¨¡å‹å·²ç»å¼ƒç”¨ï¼Œå¯ä»¥åœ¨[OpenAIç½‘ç«™](https://platform.openai.com/docs/deprecations)æ‰¾å¯ä»¥æ›¿æ¢çš„æ¨¡å‹ã€‚

è§£é‡Šä¸€ä¸‹ä»£ç ï¼š
- è°ƒç”¨ChatGPT APIï¼Œå…¶ä¸­çš„engineæ˜¯ä½¿ç”¨çš„chatgptçš„davinciå¼•æ“
- è¾“å…¥æç¤ºæ˜¯prompt
- ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦é™åˆ¶ä¸º10ä¸ªtokensï¼Œä¸€ä¸ªtokenå¯ä»¥æ˜¯ä¸€ä¸ªå•è¯æˆ–è€…ä¸€ä¸ªå­è¯
- n=1æ˜¯è®¾ç½®çš„ç”Ÿæˆæ–‡æœ¬çš„æ•°é‡ï¼Œå³è¡¨ç¤ºç”Ÿæˆä¸€æ¡å›å¤
- stopæ˜¯æ¥æŒ‡å®šç”Ÿæˆæ–‡æœ¬ç»“æŸçš„æ¡ä»¶çš„ï¼Œè¿™é‡Œè®¾ä¸ºNoneï¼Œè¡¨ç¤ºä¸è®¾ç½®åœæ­¢çš„æ¡ä»¶
- temperatureæ˜¯ç”¨æ¥æ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„éšæœºæ€§çš„ï¼Œæ•°å€¼è¶Šå¤§ä»£è¡¨ç”Ÿæˆçš„æ–‡æœ¬è¶Šéšæœºï¼Œæ•°å€¼è¶Šå°è¡¨ç¤ºç”Ÿæˆçš„æ–‡æœ¬å¾ˆç¡®å®šå’Œä¿å®ˆ
- è¿”å›çš„å†…å®¹å°±æ˜¯responseå¯¹è±¡

## 1ã€Base Model

å¤§è¯­è¨€æ¨¡å‹æ˜¯æˆ‘ä»¬ä½¿ç”¨LangChainå¼€å‘ç¨‹åºçš„åŸºç¡€ã€‚ç›®å‰ï¼Œè®¸å¤šä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨ä¸æ–­æ¶Œç°ã€‚LangChainä¸»è¦åŒºåˆ†ä¸‰ç§æ¨¡å‹ï¼š

- å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
- èŠå¤©æ¨¡å‹ï¼ˆChat Modelï¼‰
- æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆText Embedding Modelï¼‰

ä¸‹é¢æˆ‘ä»¬å°†å¯¹ä¸Šé¢çš„LLMã€Chat Modelä»¥åŠæ¨¡å‹çš„è¾“å‡ºè§£æå™¨è¿›è¡Œè¯¦ç»†è®²è§£ï¼Œå…³äºText
Embedding Modelçš„å†…å®¹ä¼šåœ¨åç»­ç« èŠ‚ä¸­å±•å¼€ä»‹ç»ã€‚

### 1.1ã€LLM

LangChainæœ¬èº«ä¸æä¾›å¤§è¯­è¨€æ¨¡å‹ï¼Œåªæä¾›å„ç§æ¥å£å’Œæ–¹å¼ç”¨ä»¥è°ƒç”¨è®¿é—®å¤§è¯­è¨€æ¨¡å‹ã€‚åœ¨LLMsä¸­ï¼Œæä¾›äº†ç»Ÿä¸€çš„æ¥å£æ¥è®¿é—®è¿™äº›å¤§æ¨¡å‹ã€‚ç°åœ¨æœ‰å¾ˆå¤šLLMæä¾›å•†æ¯”å¦‚OpenAI, Cohere, Hugging Faceç­‰ã€‚

ä»¥ä¸‹æ˜¯å£°æ˜è°ƒç”¨LLMçš„å¸¸è§æ–¹å¼ï¼Œæ¨¡å‹é€‰æ‹©æ˜¯å¾ˆå¤šçš„ï¼Œå…·ä½“è¦ä½¿ç”¨å“ªä¸ªæ¨¡å‹å¯ä»¥è®¿é—®ğŸ‘‰[è¿™é‡Œ](https://python.langchain.com/docs/integrations/llms/)ğŸ‘ˆè¿›è¡Œæµè§ˆå’Œè®¾ç½®ã€‚


```python
from langchain_openai import OpenAI

llm = OpenAI(model_name="gpt-3.5-turbo-instruct", max_tokens=1024, temperature=.7)
print(llm.invoke("Tell me a scared story."))
```

    It was a dark and stormy night, and the wind was howling outside.....


LLMæœ€åŸºæœ¬çš„åŠŸèƒ½å°±æ˜¯èƒ½å¤Ÿè°ƒç”¨å®ƒï¼Œä¼ é€’ä¸€ä¸ªå­—ç¬¦ä¸²å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚å…¶ä¸­ï¼Œ

- model_nameæ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œæˆ‘ä»¬é€‰æ‹©çš„æ¨¡å‹æ˜¯ gpt-3.5-turbo-instructã€‚
- temperatureæ˜¯OpenAIçš„Completionæ¥å£å‚æ•°ï¼Œç”¨æ¥æ§åˆ¶è¾“å‡ºç»“æœçš„ç¡®å®šæ€§ï¼šæ¸©åº¦è¶Šé«˜ï¼Œè¶Šä¸ç¡®å®šï¼›æ¸©åº¦è¶Šä½ï¼Œè¶Šç¡®å®šã€‚é»˜è®¤æ¸©åº¦æ˜¯0.7ï¼Œä¸€ä¸ªæ¯”è¾ƒå¹³è¡¡çš„æ•°å€¼ã€‚
- "tell me...story"å°±æ˜¯queryï¼Œä¼ é€’è¿›æ¨¡å‹ä¸­è°ƒç”¨é—®ç­”æ¨¡å‹ç»™å‡ºå›ç­”ï¼Œqueryçš„ç±»å‹æ˜¯å­—ç¬¦ä¸²ï¼Œæ²¡æœ‰é™åˆ¶å®ƒå¿…é¡»æ˜¯ç–‘é—®å¥è¿˜æ˜¯é™ˆè¿°å¥

### 1.2ã€Chat Model

è¿™äº›æ¨¡å‹å°†èŠå¤©æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›èŠå¤©æ¶ˆæ¯ã€‚æœ‰å…³èŠå¤©æ¨¡å‹çš„ç§ç±»ä¹Ÿå¯ä»¥ç‚¹å‡»[æ­¤å¤„](https://python.langchain.com/docs/integrations/chat/)è®¿é—®LangChainå®˜ç½‘è¿›è¡Œæµè§ˆï¼Œåœ¨æ­¤ä¸å†ä¸€ä¸€åˆ—ä¸¾ã€‚

èŠå¤©æ¨¡å‹ç•Œé¢åŸºäºæ¶ˆæ¯è€Œä¸æ˜¯åŸå§‹æ–‡æœ¬ã€‚LangChainç›®å‰æ”¯æŒçš„æ¶ˆæ¯ç±»å‹æœ‰AIMessageï¼Œ
HumanMessageï¼ŒSystemMessageå’ŒChatMessageã€‚å…¶ä¸­ï¼ŒChatMessageé€šå¸¸æ¥å—ä¸€ä¸ªä»»æ„
çš„è§’è‰²å‚æ•°ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œæˆ‘ä»¬åªéœ€å¤„ç†HumanMessageï¼ŒAIMessageå’ŒSystemMessageã€‚
å®ƒä»¬å¯¹åº”äº†OpenAIèŠå¤©æ¨¡å‹APIæ”¯æŒçš„ä¸åŒè§’è‰²userã€assistantå’Œsystemã€‚

```python
from langchain_openai import ChatOpenAI
from langchain import PromptTemplate, LLMChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(temperature=0)
# #humanç»™å‡ºçš„ä¿¡æ¯æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å†…å®¹ï¼Œç»™åˆ°chatä¹‹åä¼šè¿”å›ä¸€ä¸ªAIMessageä¿¡æ¯ï¼Œå…¶ä¸­çš„å†…å®¹å°†æ˜¯ç»™å‡ºçš„åé¦ˆç­”æ¡ˆ
chat.invoke([HumanMessage(content="Translate this sentence from English to Chinese. I love programming.")])
```


    AIMessage(content='æˆ‘çˆ±ç¼–ç¨‹ã€‚')

é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†SystemMessageçš„å†…å®¹è®¾å®šä¸ºä¸ºAIèµ‹äºˆä¸€ä¸ªè§’è‰²ï¼Œè¿™æ ·OpenAIæ¨¡å‹å¯ä»¥æ›´å¥½åœ°åœ¨ç‰¹å®šé¢†åŸŸå†…æä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼ŒOpenAIçš„èŠå¤©æ¨¡å‹æ”¯æŒåŒæ—¶è¾“å…¥æ‰¹é‡ä¿¡æ¯ã€‚æ‚¨å¯ä»¥ä½¿ç”¨chat.generateæ–¹æ³•æ¥å¤„ç†å¤šè½®å¯¹è¯å†…å®¹ï¼Œè¿”å›å€¼LLMResultä¸­åŒ…å«äº†è¾“å…¥ä¿¡æ¯ã€‚


```python
batch_messages = [
    [
    	SystemMessage(content="You are a helpful assistant that translates English to French."),
    	HumanMessage(content="Translate this sentence from English to French. I love programming.")
    ],
    [
    	SystemMessage(content="You are a helpful assistant that translates English to Chinese."),
    	HumanMessage(content="Translate this sentence from English to Chinese. I love artificial intelligence.")
    ],
]
result = chat.generate(batch_messages)
print(result.generations[0][0].text)
print(result.generations[1][0].text)
```

    J'adore programmer.
    æˆ‘å–œæ¬¢äººå·¥æ™ºèƒ½ã€‚

å½“ç„¶ï¼Œç»“åˆLLMChainå’ŒPromptã€Chat modelï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°å¼€å±•å¯¹è¯ï¼Œè¿™ä¸€éƒ¨åˆ†çš„ä¾‹å­
ä¼šåœ¨åé¢çš„å†…å®¹ä¸­è¯¦ç»†ä»‹ç»ã€‚

### 1.3ã€Output Parsers

è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºçš„å†…å®¹é€šå¸¸æ˜¯æ–‡æœ¬æ ¼å¼ã€‚ç„¶è€Œï¼Œåœ¨å¼€å‘äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºæ—¶ï¼Œæˆ‘ä»¬é€š
å¸¸éœ€è¦æ›´åŠ æ ¼å¼åŒ–çš„å†…å®¹ï¼Œä¾‹å¦‚å°†ç»“æœè½¬æ¢ä¸ºç›®æ ‡å¯¹è±¡æˆ–æ•°ç»„ç­‰ï¼Œä»¥ä¾¿ç¨‹åºæ›´å¥½åœ°å¤„ç†ã€‚ä¸º
äº†æ»¡è¶³è¿™ä¸ªéœ€æ±‚ï¼ŒLangChainæä¾›äº†è¾“å‡ºè§£æå™¨ï¼ˆOutput parserï¼‰æ¥æ ¼å¼åŒ–æ¨¡å‹è¿”å›çš„å†…
å®¹ã€‚è¾“å‡ºè§£æå™¨çš„ä¸»è¦ä½œç”¨æ˜¯å°†è¯­è¨€æ¨¡å‹è¿”å›çš„ç»“æœè¿›è¡Œæ ¼å¼åŒ–ã€‚ä¸€ä¸ªè¾“å‡ºè§£æå™¨å¿…é¡»å®ç°ä»¥ä¸‹ä¸¤ä¸ªå…³é”®æ–¹æ³•ï¼š

- get_format_instructions: è¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå…¶ä¸­åŒ…å«äº†æŒ‡å¯¼è¯­è¨€æ¨¡å‹åº”è¯¥
è¿”å›ä»€ä¹ˆæ ¼å¼å†…å®¹çš„æç¤ºè¯ã€‚å®ƒå‘Šè¯‰æ¨¡å‹åº”è¯¥å¦‚ä½•æ„å»ºå…¶è¾“å‡ºï¼Œä»¥ç¡®ä¿è¾“å‡ºçš„å†…å®¹
å¯ä»¥è¢«åç»­çš„è§£æå™¨æ­£ç¡®å¤„ç†ã€‚
- parseï¼šè¿™ä¸ªæ–¹æ³•ç”¨äºå°†è¯­è¨€æ¨¡å‹è¿”å›çš„å†…å®¹è§£ææˆç›®æ ‡æ ¼å¼ã€‚å®ƒå°†æ¨¡å‹çš„è¾“å‡ºè½¬
åŒ–ä¸ºæ›´ç»“æ„åŒ–çš„å½¢å¼ï¼Œä»¥ä¾¿åç»­çš„å¤„ç†å’Œä½¿ç”¨ã€‚

è¿˜æœ‰ä¸€ä¸ªå¯é€‰çš„æ–¹æ³•ï¼š
- Parse with promptï¼šè¿™ä¸ªæ–¹æ³•æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆå‡å®šä¸ºè¯­è¨€æ¨¡å‹çš„å“åº”ï¼‰å’Œä¸€ä¸ª
æç¤ºï¼ˆå‡å®šä¸ºç”Ÿæˆè¿™ä¸ªå“åº”çš„æç¤ºï¼‰ï¼Œå¹¶å°†å…¶è§£ææˆæŸç§ç»“æ„ã€‚æç¤ºä¸»è¦ç”¨äºåœ¨è¾“
å‡ºè§£æå™¨éœ€è¦é‡è¯•æˆ–ä¿®å¤è¾“å‡ºæ—¶æä¾›ä¿¡æ¯ï¼Œä»¥ä¾¿æ ¹æ®æç¤ºçš„å†…å®¹è¿›è¡Œç›¸åº”çš„æ“ä½œã€‚


```python
from typing import List
from langchain_openai import OpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import BaseModel, Field, validator


model = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.0)

# å®šä¹‰æ‚¨æœŸæœ›çš„æ•°æ®ç»“æ„
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

    # ä½¿ç”¨ Pydantic è½»æ¾æ·»åŠ è‡ªå®šä¹‰éªŒè¯é€»è¾‘
    @validator('setup')
    def question_ends_with_question_mark(cls, field):
        if field[-1] != "?":
            raise ValueError("question should end with a question mark")
        return field
    

# åˆå§‹åŒ– PydanticOutputParser è§£æå™¨
parser = PydanticOutputParser(pydantic_object=Joke)

# è®¾ç½®æç¤ºæ¨¡æ¿
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# ç»„åˆæç¤ºæ¨¡æ¿å’Œè¯­è¨€æ¨¡å‹
prompt_and_model = prompt | model

# æäº¤ç”¨æˆ·æŸ¥è¯¢å¹¶è§£æå“åº”
output = prompt_and_model.invoke({"query": "Tell me a joke."})
parser.invoke(output)
```


    Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `template="Answer the user query.\n{format_instructions}\n{query}\n"`ï¼Œå…¶ä¸­format_instructionsæ˜¯æŒ‡å¯¼è¯­è¨€æ¨¡å‹åº”è¯¥è¿”å›ä»€ä¹ˆæ ¼å¼å†…å®¹çš„æç¤ºè¯ï¼Œqueryæ˜¯ç”¨æˆ·çš„é—®é¢˜ã€‚è¿™ä¸ªæ¨¡æ¿æ˜¯ç”¨æ¥å‘Šè¯‰æ¨¡å‹åº”è¯¥å¦‚ä½•æ„å»ºå…¶è¾“å‡ºï¼Œä»¥ç¡®ä¿è¾“å‡ºçš„å†…å®¹å¯ä»¥è¢«åç»­çš„è§£æå™¨æ­£ç¡®å¤„ç†ã€‚

queryåœ¨æˆ‘ä»¬çš„ä»£ç ä¸­å³ `"Tell me a joke."`

å¦‚æœè¦æŸ¥çœ‹`format_instructions`çš„å†…å®¹ï¼Œå¯ä»¥ä½¿ç”¨`print(parser.get_format_instructions())`è¿›è¡Œæ‰“å°ã€‚


```python
print(parser.get_format_instructions())
```

    The output should be formatted as a JSON instance that conforms to the JSON schema below.
    
    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
    
    Here is the output schema:
    ```
    {"properties": {"setup": {"title": "Setup", "description": "question to set up a joke", "type": "string"}, "punchline": {"title": "Punchline", "description": "answer to resolve the joke", "type": "string"}}, "required": ["setup", "punchline"]}
    ```


## 2ã€Prompt

Promptä¸ä»…å¯ä»¥å‘æŒ¥LLMçš„å¼ºå¤§èƒ½åŠ›ï¼Œè¿˜å¯ä»¥å°†å…¶åº”ç”¨äºå„ç§å®é™…åœºæ™¯ã€‚é€šè¿‡Promptï¼Œä½ å¯ä»¥å¼•å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€å›ç­”é—®é¢˜ï¼Œç”šè‡³å‚ä¸å¯¹è¯ã€‚

åœ¨è¿™ä¸€ç« èŠ‚ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨Promptçš„æ¦‚å¿µã€å¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„æç¤ºå·¥ç¨‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨
LangChainä¸­çš„æç¤ºæ¨¡æ¿æ¥ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚Promptä¸ºæˆ‘ä»¬æä¾›äº†æ›´å¤šçš„æ§åˆ¶åŠ›ï¼Œä½¿æˆ‘ä»¬èƒ½
å¤Ÿæ›´ç²¾ç¡®åœ°å¼•å¯¼æ¨¡å‹ä»¥æ»¡è¶³å„ç§éœ€æ±‚ã€‚

### 2.1ã€Prompt Engineering

Prompt Engineeringæ˜¯ä¸€ç§åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡è®¾è®¡ã€ä¼˜åŒ–å’Œè¯„ä¼°è¾“å…¥æç¤º
ï¼ˆpromptï¼‰æ¥å¼•å¯¼å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—æ¨¡å‹ï¼‰ç”ŸæˆæœŸæœ›çš„è¾“å‡ºã€‚ç®€è€Œè¨€ä¹‹ï¼Œ Prompt Engineering å°±æ˜¯æ‰¾åˆ°æœ€ä½³æ–¹å¼å‘æ¨¡å‹æé—®ï¼Œä»¥è·å¾—æœ€æœ‰ç”¨ã€æœ€å‡†ç¡®çš„å›ç­”ã€‚

è®¾è®¡ä¸€ä¸ªé€šç”¨çš„promptæ¡†æ¶é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹ä¸€äº›è¦ç´ ï¼š

- èƒŒæ™¯ä»‹ç»ï¼šåœ¨æç¤ºä¸­ï¼Œé€šå¸¸éœ€è¦æä¾›ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æˆ–èƒŒæ™¯ä¿¡æ¯ã€‚è¿™æœ‰åŠ©äºç¡®ä¿æ¨¡å‹
ç†è§£é—®é¢˜çš„èƒŒæ™¯å’Œç›¸å…³é¢†åŸŸã€‚èƒŒæ™¯ä»‹ç»éƒ¨åˆ†å¯ä»¥åŒ…æ‹¬é—®é¢˜çš„å†å²ã€ç›¸å…³äº‹ä»¶æˆ–å…¶ä»–
ç›¸å…³ä¿¡æ¯ã€‚
- ä»»åŠ¡æŒ‡ä»¤ï¼šä»»åŠ¡æŒ‡ä»¤éƒ¨åˆ†åŒ…å«æ˜ç¡®çš„æŒ‡å¯¼æˆ–ä»»åŠ¡æè¿°ã€‚å®ƒå‘Šè¯‰æ¨¡å‹å®ƒéœ€è¦æ‰§è¡Œçš„ä»»
åŠ¡ï¼Œé€šå¸¸ä»¥æ˜ç¡®çš„å‘½ä»¤æˆ–é—®é¢˜çš„å½¢å¼è¡¨ç¤ºã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿æ¨¡å‹ç†è§£ä»»åŠ¡çš„å…³é”®è¦
ç‚¹ã€‚
- å‚è€ƒä¿¡æ¯ï¼šä¸ºäº†å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ‰§è¡Œä»»åŠ¡ï¼Œé€šå¸¸æä¾›ä¸ä»»åŠ¡ç›¸å…³çš„å‚è€ƒä¿¡æ¯æˆ–ä¸Šä¸‹
æ–‡ã€‚è¿™å¯ä»¥æ˜¯å…·ä½“çš„æ•°æ®ã€æ–‡çŒ®å¼•ç”¨ã€æˆ–å…¶ä»–æ”¯æŒææ–™ã€‚å‚è€ƒä¿¡æ¯æœ‰åŠ©äºæ¨¡å‹ç”Ÿæˆ
æ›´å‡†ç¡®å’Œæœ‰æ ¹æ®çš„å›åº”ã€‚
- è¾“å‡ºçº¦æŸï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦å¯¹æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºè¿›è¡Œçº¦æŸã€‚è¿™å¯ä»¥åŒ…æ‹¬ç‰¹å®š
çš„æ ¼å¼ã€ç­”æ¡ˆçš„é•¿åº¦ã€æˆ–å…¶ä»–ç”Ÿæˆç»“æœçš„è¦æ±‚ã€‚è¾“å‡ºçº¦æŸæœ‰åŠ©äºç¡®ä¿æ¨¡å‹çš„å›åº”æ»¡
è¶³ç‰¹å®šæ ‡å‡†ã€‚
- åˆ—ä¸¾ä¾‹å­ï¼šä¸ºäº†æ›´æ¸…æ¥šåœ°è¯´æ˜ä»»åŠ¡å’ŒæœŸæœ›çš„è¾“å‡ºï¼Œé€šå¸¸ä¼šæä¾›ç¤ºä¾‹ã€‚è¿™äº›ç¤ºä¾‹æ˜¯ä»»
åŠ¡çš„å…¸å‹ç¤ºä¾‹ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£ä»»åŠ¡çš„ç±»å‹å’Œé¢„æœŸçš„å“åº”ã€‚


### 2.2ã€Prompt Template

LangChainæä¾›äº†Prompt Templateï¼Œæç¤ºæ¨¡æ¿å…è®¸æˆ‘ä»¬å°†æ ¼å¼åŒ–æç¤ºçš„é€»è¾‘ä¸å®é™…çš„è¯­è¨€
æ¨¡å‹è°ƒç”¨åˆ†ç¦»å¼€æ¥ã€‚ç°åœ¨æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨Prompt Templatesçš„ç®€å•ç”¨ä¾‹ï¼š


```python
from langchain import PromptTemplate
from langchain_openai import OpenAI

def generate_city_names(city_features):
    #å…·ä½“çš„æ¨¡ç‰ˆå†…å®¹ï¼Œå…¶ä¸­è¦è¿›è¡Œè¡¥å…¨çš„åœ°æ–¹ç”¨å¤§æ‹¬å·è¿›è¡Œå˜é‡çš„æ”¾ç½®ï¼Œä¸éœ€è¦å…¶ä»–çš„æ“ä½œï¼Œç±»ä¼¼äºå­—ç¬¦ä¸²ä¸­å¯¹äºæŸä¸€ä¸ªå˜é‡å¯¹å…¶çš„formatæ ¼å¼åŒ–
    prompt_template = "I would like to travel to other cities. The main features of cities are:{}ã€‚Give me three cities by nameonly."

    #å°†æç¤ºåŒ…è£…æˆå­—ç¬¦ä¸²åˆ—è¡¨
    prompt = [prompt_template.format(city_features)]
    llm = OpenAI()
    response = llm.generate(prompt, max_tokens=100, temperature=0.8)
    city_names = [gen[0].text.strip() for gen in response.generations]
    return city_names

city_features = "Sun, beach, romance."
city_names = generate_city_names(city_features)
print(city_names)
```

    ['1. Miami, Florida\n2. Honolulu, Hawaii\n3. Barcelona, Spain']


ä»ä¸Šé¢çš„ç”¨ä¾‹ä¸­å¯ä»¥çœ‹åˆ°ï¼šé€šè¿‡LangChainçš„PromptTemplateç±»ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå…·ä½“çš„
promptï¼ŒPromptTemplateæ¥å—å¤šä¸ªè¾“å…¥å˜é‡ï¼Œç”¨æ¥æ ¼å¼åŒ–ç”Ÿæˆpromptã€‚


```python
from langchain import PromptTemplate

#input_variablesçš„å€¼å¯ä»¥ä¸ºç©ºï¼Œè¯´æ˜å…¶ä¸­æ²¡æœ‰ä»»ä½•å˜é‡
no_input_prompt = PromptTemplate(input_variables=[], template="Tell me a joke.")
no_input_prompt.format()

#ä¸€ä¸ªinput_variableçš„ç¤ºä¾‹ï¼Œè¿™æ ·æ¨¡ç‰ˆåŒ–ä¹‹åçš„æç¤ºå°†æŠŠadjectiveä½œä¸ºå‚æ•°ä¼ å…¥
one_input_prompt = PromptTemplate(input_variables=["adjective"],
template="Tell me a {adjective} joke.")
one_input_prompt.format(adjective="funny")

#å¤šä¸ªinput_variablesçš„ç¤ºä¾‹ï¼Œæ¨¡ç‰ˆåçš„æç¤ºå°†adjectiveå’Œcontentä½œä¸ºå‚æ•°ä¼ å…¥
multiple_input_prompt = PromptTemplate(
    input_variables=["adjective", "content"],
    template="Tell me a {adjective} joke about {content}."
)
multiple_input_prompt.format(adjective="funny", content="chickens")
```


    'Tell me a funny joke about chickens.'

æ­¤å¤–ï¼Œè¿˜å¯ä»¥ è‡ªå®šä¹‰Prompt Templateã€ Few-shot Prompt Templates

## 3ã€Chat Model

èŠå¤©æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´å­˜åœ¨çš„ä¸€ä¸ªé‡è¦å·®å¼‚â€”â€”èŠå¤©æ¨¡å‹å…·æœ‰å­˜å‚¨èŠå¤©è®°å½•çš„åŠŸèƒ½ã€‚è¿™ä¸ªç‰¹æ€§ä¸ºèŠå¤©æ¨¡å‹å¢åŠ äº†ä¸€å±‚æ™ºèƒ½å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œè®©å®ƒèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”ç”¨æˆ·çš„é—®é¢˜ã€‚é€šè¿‡å­˜å‚¨å…ˆå‰çš„å¯¹è¯å†…å®¹ï¼ŒèŠå¤©æ¨¡å‹å¯ä»¥å®ç°æ›´è¿è´¯ã€æ›´æ™ºèƒ½çš„å¯¹è¯ï¼Œä¸ä»…å›ç­”å½“å‰çš„é—®é¢˜ï¼Œè¿˜è€ƒè™‘åˆ°æ•´ä¸ªå¯¹è¯å†å²ï¼Œä½¿å¾—äº¤äº’æ›´åŠ è‡ªç„¶å’Œæœ‰è¶£ã€‚

###  3.1ã€Chat Prompt

èŠå¤©æ¨¡å‹çš„æç¤ºæ˜¯åŸºäºæ¶ˆæ¯è€Œæ„å»ºçš„ï¼Œè€Œä¸ä»…ä»…æ˜¯çº¯æ–‡æœ¬ã€‚

å¯ä»¥é‡‡ç”¨MessagePromptTemplateæ¥ä¸ºæç¤ºæ·»åŠ æ¨¡æ¿åŒ–çš„æ•ˆæœã€‚å€ŸåŠ©ä¸€ä¸ªæˆ–å¤šä¸ª
MessagePromptTemplatesï¼Œæˆ‘ä»¬è¿˜èƒ½æ„å»ºChatPromptTemplateã€‚åœ¨è¿™ä¹‹åï¼Œä½¿ç”¨
ChatPromptTemplateçš„format_promptæ–¹æ³•å³å¯è·å¾—PromptValueï¼Œç„¶åæ ¹æ®éœ€è¦å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯å¯¹è±¡ã€‚è¿™å®Œå…¨å–å†³äºæˆ‘ä»¬æ˜¯å¸Œæœ›å°†è¿™ä¸ªç»è¿‡æ ¼å¼åŒ–çš„å€¼ç”¨ä½œ LLMè¿˜æ˜¯èŠå¤©æ¨¡å‹çš„è¾“å…¥ã€‚

ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œè¯¥æ¨¡æ¿ä¸Šå®šä¹‰äº†ä¸€ä¸ªfrom_templateæ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š


```python
from langchain_openai import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

chat = ChatOpenAI()

template = "You are a helpful assistant that translates {input_language} to {output_language}."

# åˆ›å»ºä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯æ¨¡æ¿ï¼Œç”¨äºè®¾å®šèŠå¤©å¯¹è¯ä¸­çš„ç³»ç»Ÿæ¶ˆæ¯
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
# åˆ›å»ºä¸€ä¸ªäººç±»æ¶ˆæ¯æ¨¡æ¿ï¼Œç”¨äºå®šä¹‰äººç±»è¾“å…¥çš„æ¶ˆæ¯
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
# åˆ›å»ºä¸€ä¸ª ChatPromptTemplate å®ä¾‹ï¼Œä½¿ç”¨ç³»ç»Ÿæ¶ˆæ¯å’Œäººç±»æ¶ˆæ¯æ¨¡æ¿
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# é€šè¿‡å¯¹æ ¼å¼åŒ–çš„æ¶ˆæ¯æ‰§è¡Œ chat æ–¹æ³•ï¼Œè·å–èŠå¤©å®Œæˆ
print(chat.invoke(chat_prompt.format_prompt(input_language="English",output_language="French", text="I love programming.").to_messages()))
```

    content="J'adore la programmation."


å¦‚æœæƒ³è¦æ›´ç›´æ¥åœ°æ„å»ºMessagePromptTemplateï¼Œå¯ä»¥åœ¨å¤–éƒ¨åˆ›å»ºä¸€ä¸ª PromptTemplateç„¶åå°†å…¶ä¼ å…¥ï¼Œä¾‹å¦‚ï¼š


```python
prompt=PromptTemplate(
    template="You are a helpful assistant that translates {input_language} to {output_language}.",
    input_variables=["input_language", "output_language"],
)
system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)
```

### 3.2ã€Memory

Chat Modelä¸€ä¸ªå¼•äººæ³¨ç›®çš„ç‰¹ç‚¹æ˜¯å®ƒä»¬çš„èŠå¤©è®°å½•å¯ä»¥å…·æœ‰è®°å¿†å­˜å‚¨ã€‚è¿™æ„å‘³ç€Chat Modelå¯ä»¥åœ¨å¯¹è¯çš„ä¸åŒéƒ¨åˆ†ä¸­ä¿æŒå¯¹ä¹‹å‰å‘ç”Ÿçš„äº‹ä»¶å’Œä¿¡æ¯çš„ä¸Šä¸‹æ–‡è®°å¿†ã€‚è¿™ç§åŠŸèƒ½ä½¿Chat Modelæ›´æ™ºèƒ½ã€æ›´è´´è¿‘çœŸå®å¯¹è¯ï¼Œä½†å®ƒä¹Ÿå¼•å‡ºäº†ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜ï¼šå¦‚ä½•åœ¨æ¨¡å‹ä¹‹é—´æˆ–åœ¨å¯¹è¯çš„ä¸åŒé˜¶æ®µå…±äº«è¿™ç§è®°å¿†ï¼Ÿ

LangChainå¼•å…¥äº†Memoryæ¨¡å—ï¼Œå®ƒå…è®¸æˆ‘ä»¬æ›´çµæ´»åœ°ç®¡ç†å’Œå…±äº«å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚è¿™ä¸ªæ¨¡å—
å°†ä¿¡æ¯å­˜å‚¨åœ¨å†…éƒ¨è®°å¿†ä¸­ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶æ£€ç´¢ã€‚ä»Chat Modelçš„èŠå¤©è®°å½•ä¸­æŠ½å–å¹¶å­˜å‚¨ä¿¡
æ¯ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™å…¶ä»–ç»„ä»¶æˆ–æ¨¡å‹ï¼Œä»¥åœ¨æ•´ä¸ªåº”ç”¨ç¨‹åºä¸­ä¿æŒä¸€è‡´çš„ä¸Šä¸‹æ–‡ï¼Œæ˜¯Memory
æ¨¡å—çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚

å¯¹è¯çš„å†å²è®°å½•ä½œä¸ºpromptæ¡†æ¶ä¸Šçš„å‚è€ƒä¿¡æ¯éœ€è¦ä¸€èµ·é€ç»™æ¨¡å‹å›ç­”é—®é¢˜ã€‚ç„¶è€Œå¤§æ¨¡å‹çš„
è°ƒç”¨éƒ½æ˜¯æœ‰çª—å£é•¿åº¦é™åˆ¶çš„ï¼Œæ‰€ä»¥è¿™ä¸ªå†å²è®°å½•ä¸èƒ½æ— é™çš„é•¿ã€‚å› æ­¤ï¼ŒLangChainå¼•å…¥
Memoryï¼Œæ¥å¯¹å†å²è®°å½•è¿›è¡Œç®¡ç†ã€‚ä¸åŒçš„Memoryçš„å‡½æ•°ï¼Œå°±æ˜¯ä¸åŒçš„å½¢æˆpromptå‚è€ƒä¿¡æ¯çš„æ–¹æ³•ã€‚

å¯¹è¯ç¼“å†²åŒºå†…å­˜ConversationBufferMemoryå¯ä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾åˆ›å»ºå¯¹è¯å†å²ï¼Œå®ƒä¼šå°†èŠå¤©å†…å®¹è®°å½•åœ¨å†…å­˜ä¸­ï¼Œè€Œä¸éœ€è¦æ¯æ¬¡å†æ‰‹åŠ¨æ‹¼æ¥èŠå¤©è®°å½•ï¼Œç”¨æ³•å¦‚ä¸‹:


```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("whats up?")
#åŠ è½½å¯¹è¯ç¼“å­˜ä¸­å­˜å‚¨çš„å˜é‡ã€‚è¯¥æ–¹æ³•æ¥å—ä¸€ä¸ªç©ºå­—å…¸ä½œä¸ºå‚æ•°ï¼Œè¡¨ç¤ºä¸éœ€è¦åŠ è½½ä»»ä½•å˜é‡
memory.load_memory_variables({})
#ä¹Ÿå¯ä»¥ç”¨åˆ—è¡¨è¿”å›å†å²æ¶ˆæ¯
memory = ConversationBufferMemory(return_messages=True)
memory.chat_memory.add_user_message("hi!")
memory.chat_memory.add_ai_message("whats up?")
memory.load_memory_variables({})
```


    {'history': [HumanMessage(content='hi!'), AIMessage(content='whats up?')]}

æ¥ä¸‹æ¥æˆ‘ä»¬é€šè¿‡å‡ ä¸ªç¤ºä¾‹çœ‹ä¸€ä¸‹å…·ä½“æ€ä¹ˆå°†å…¶æ·»åŠ åˆ°Chainä¸­.


```python
from langchain_openai import OpenAI
from langchain.chains import ConversationChain
from langchain import LLMChain, PromptTemplate

template = """You are a chatbot having a conversation with a human.
{chat_history}
Human: {human_input}
Chatbot:"""

prompt = PromptTemplate(
    input_variables=["chat_history", "human_input"],
    template=template
)

memory = ConversationBufferMemory(memory_key="chat_history")

llm_chain = LLMChain(
    llm=OpenAI(),
    prompt=prompt,
    verbose=True,
    memory=memory,
)

llm_chain.predict(human_input="Hi there!")
```

    ' Hello! How are you doing today?'


```python
llm_chain.predict(human_input="I'm doing well! Just having a conversation with an AI.")
```

    " That's great to hear! I am always happy to have conversations with humans. What would you like to talk about?"


```python
llm_chain.predict(human_input="Tell me about yourself.")
```

    ' I am an AI chatbot designed to interact and communicate with humans....'

### 3.3ã€Cache

å‘ openAI è°ƒç”¨æ¥å£éƒ½æ˜¯è¦èŠ±é’±çš„ï¼Œå¦‚æœç”¨æˆ·é—®åŒä¸€ä¸ªé—®é¢˜ï¼Œå¯¹ç»“æœè¿›è¡Œäº†ç¼“å­˜ï¼Œè¿™æ ·å°±å¯ä»¥å‡å°‘æ¥å£çš„è°ƒç”¨å¹¶ä¸”ä¹Ÿèƒ½åŠ å¿«æ¥å£è¿”å›çš„é€Ÿåº¦ã€‚

ç¼“å­˜æ˜¯æé«˜æ•ˆç‡çš„å¼ºå¤§å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é‡é‡å¤éœ€æ±‚æ—¶ã€‚ç¼“å­˜çš„é€‰æ‹©å–å†³äºä¸åŒéœ€æ±‚ï¼Œ
åŒ…æ‹¬å†…å­˜ç¼“å­˜ã€SQLiteç¼“å­˜ã€Redisç¼“å­˜ã€è‡ªå®šä¹‰SQLAlchemyç¼“å­˜ç­‰ã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä½¿ç”¨å†…å­˜ç¼“å­˜çš„ç”¨ä¾‹ï¼Œå†…å­˜ç¼“å­˜é€‚ç”¨äºçŸ­æš‚çš„ç¼“å­˜éœ€æ±‚ã€‚å®ƒçš„ä¼˜åŠ¿åœ¨äºé€Ÿåº¦æå¿«ï¼Œå¯ä»¥åœ¨æçŸ­æ—¶é—´å†…è®¿é—®ç¼“å­˜æ•°æ®ã€‚é€šå¸¸ï¼Œå†…å­˜ç¼“å­˜æœ‰ä¸€å®šé™åˆ¶ï¼Œå½“è¾¾åˆ°é™åˆ¶æ—¶ï¼Œè¾ƒæ—©çš„ç¼“å­˜æ•°æ®å°†è¢«åˆ é™¤ï¼š


```python
import langchain
from langchain_openai import OpenAI
from langchain.cache import InMemoryCache

langchain.llm_cache = InMemoryCache()
# To make the caching really obvious, lets use a slower model.
llm = OpenAI(model_name="gpt-3.5-turbo-instruct", n=2, best_of=2)
# The first time, it is not yet in cache, so it should take longer
print(llm.invoke("Tell me a joke"))
# The second time it is, so it goes faster
print(llm.invoke("Tell me a joke"))
```


    Why was the math book sad?
    Because it had too many problems.
    
    Why was the math book sad?
    Because it had too many problems.


å½“éœ€è¦ç¼“å­˜çš„æ•°æ®è¾ƒå¤§æˆ–éœ€è¦é•¿æœŸæŒä¹…æ€§æ—¶ï¼ŒSQLiteæ˜¯ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©ã€‚å®ƒæä¾›äº†æ›´å¥½çš„æ”¯æŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç®¡ç†è¾ƒå¤§çš„ç¼“å­˜æ•°æ®ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨SQLiteç¼“å­˜çš„ä¸€ä¸ªç¤ºä¾‹ï¼š


```python
import langchain
from langchain_openai import ChatOpenAI
from langchain.cache import SQLiteCache

# è®¾ç½®è¯­è¨€æ¨¡å‹çš„ç¼“å­˜æ•°æ®å­˜å‚¨çš„åœ°å€
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")
# åŠ è½½ llm æ¨¡å‹
llm = ChatOpenAI()
# ç¬¬ä¸€æ¬¡å‘æ¨¡å‹æé—®
result = llm.invoke('tell me a joke')
print(result)
# ç¬¬äºŒæ¬¡å‘æ¨¡å‹æé—®åŒæ ·çš„é—®é¢˜
result2 = llm.invoke('tell me a joke')
print(result2)
```

    content='Why did the scarecrow win an award? Because he was outstanding in his field!'
    content='Why did the scarecrow win an award? Because he was outstanding in his field!'


ä½¿ç”¨ç¼“å­˜å¯ä»¥æ˜¾è‘—æé«˜ç¨‹åºçš„æ•ˆç‡ï¼Œä½¿ä»£ç è¿è¡Œæ›´å¿«ä¸”æ›´æµç•…ã€‚ä¸åŒç±»å‹çš„ç¼“å­˜é€‚ç”¨äºä¸åŒ
åœºæ™¯ï¼Œä»å†…å­˜ç¼“å­˜åˆ°æŒä¹…æ€§çš„SQLiteç¼“å­˜å’ŒRedisç¼“å­˜ï¼Œå†åˆ°åŸºäºè¯­ä¹‰å’ŒGPTçš„æ™ºèƒ½ç¼“å­˜ï¼Œ
éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç”¨é€”ã€‚ SQLAlchemyCache åˆ™æ”¯æŒä»»ä½•SQLAlchemyæ”¯æŒçš„SQLæ•°æ®åº“ã€‚å› 
æ­¤ï¼Œç¼“å­˜åœ¨LangChainä¸­æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºæé«˜æ•ˆç‡å’ŒåŠ é€Ÿä»£ç çš„è¿è¡Œã€‚

## 4ã€Chain

é€šè¿‡å‰é¢çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨Promptsä¸æ¨¡å‹è¿›è¡Œäº’åŠ¨ã€‚è¿™äº›æ¦‚å¿µä¸ºæˆ‘ä»¬æ„å»º
å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

ä½†è¦çœŸæ­£å°†è¿™äº›æ¦‚å¿µè½¬åŒ–ä¸ºæœ‰ç”¨çš„å·¥å…·å’Œåº”ç”¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥å°†å®ƒä»¬æœ‰æœºåœ°è¿æ¥åœ¨ä¸€
èµ·ï¼Œåˆ›é€ å‡ºæ›´å¤æ‚çš„ã€åŠŸèƒ½æ›´å¼ºå¤§çš„ç³»ç»Ÿã€‚æ­£æ˜¯åœ¨è¿™é‡Œï¼ŒLangChainçš„é“¾ï¼ˆChainï¼‰å……å½“äº†
ä¸€ä¸ªç²˜åˆå‰‚ï¼Œå°†è¯­è¨€æ¨¡å‹ã€Promptsä»¥åŠå…¶ä»–ç»„ä»¶ä¸²è”åœ¨ä¸€èµ·ï¼Œæ„å»ºå‡ºååŒå·¥ä½œçš„æ™ºèƒ½ç³»
ç»Ÿã€‚

é“¾å…è®¸æˆ‘ä»¬å°†å¤šä¸ªç»„ä»¶ç»„åˆåœ¨ä¸€èµ·ï¼Œåˆ›å»ºä¸€ä¸ªå•ä¸€çš„ã€ä¸€è‡´çš„åº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å¤š
ä¸ªé“¾ç»„åˆåœ¨ä¸€èµ·ï¼Œæˆ–è€…é€šè¿‡å°†é“¾ä¸å…¶ä»–ç»„ä»¶ç»„åˆåœ¨ä¸€èµ·ï¼Œæ¥æ„å»ºæ›´å¤æ‚çš„é“¾ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯
ä»¥åˆ›å»ºä¸€ä¸ªé“¾ï¼Œè¯¥é“¾æ¥æ¥å—ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨PromptTemplateå¯¹å…¶è¿›è¡Œæ ¼å¼åŒ–ï¼Œç„¶åå°†æ ¼å¼
åŒ–åçš„å“åº”ä¼ é€’ç»™LLMã€‚

### 4.1ã€LLMChian

LLMChainï¼Œå®ƒæ˜¯ä¸€ä¸ªæœ€ç®€å•ã€ä½¿ç”¨æœ€å¤šçš„chainï¼Œå®ƒä¼šæ¥æ”¶ä¸€ä¸ªpromptæ¨¡æ¿ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™
ä¸ªæ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥ä¿¡æ¯ï¼Œç„¶åè¿”å›ç”¨æˆ·çš„æŸ¥è¯¢å¾—åˆ°çš„å“åº”ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç­”æ¡ˆã€‚ä¸‹é¢æ˜¯
ä½¿ç”¨ä¾‹å­ï¼š


```python
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI


llm = OpenAI(temperature=0.9)
#æç¤ºæ¨¡ç‰ˆæ ¼å¼åŒ–å˜é‡å†…å®¹
prompt = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes{product}?",
)

from langchain.chains import LLMChain
#å®ä¾‹åŒ–chainå¯¹è±¡
chain = LLMChain(llm=llm, prompt=prompt)
#ç”¨chainçš„runæ–¹æ³•æ¥è¿è¡ŒæŒ‡å®šè¾“å…¥å˜é‡çš„é“¾
print(chain.invoke("colorful socks"))
```

    {'product': 'colorful socks', 'text': '\n\n"Rainbow Sox Co."'}


åœ¨LLMChainä¸­ä½¿ç”¨chat modelå»å®ŒæˆèŠå¤©æœºå™¨äººçš„äº¤äº’è¿‡ç¨‹ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š


```python
from langchain_openai import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)

human_message_prompt = HumanMessagePromptTemplate(
    prompt=PromptTemplate(
        template="What is a good name for a company that makes {product}?",
        input_variables=["product"],
    )
)
chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])
chat = ChatOpenAI(temperature=0.9)
chain = LLMChain(llm=chat, prompt=chat_prompt_template)
print(chain.invoke("colorful socks"))

```

    {'product': 'colorful socks', 'text': 'Rainbow Feet Co.'}

ä¸‹é¢è¿™æ®µä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨LangChainåˆ›å»ºä¸€ä¸ªåŒ…å«OpenAIè¯­è¨€æ¨¡å‹å’Œ
ConversationBufferMemoryè®°å¿†çš„å¯¹è¯é“¾ã€‚åœ¨è¿™ä¸ªé“¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ConversationChainç±»ï¼Œè®¾ç½®verbose=Trueä»¥è¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼Œç„¶åè¾“å…¥"Hi there!"å¹¶è·å–ç›¸åº”çš„é¢„æµ‹ç»“æœã€‚


```python
from langchain_openai import OpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

llm = OpenAI(temperature=0)
# åˆ›å»ºä¸€ä¸ª ConversationChain å®ä¾‹ï¼Œè¿™é‡ŒåŒ…å«äº† OpenAI è¯­è¨€æ¨¡å‹å’ŒConversationBufferMemory è®°å¿†
conversation = ConversationChain(
    llm=llm,
    verbose=True, # è®¾ç½® verbose ä¸º True ä»¥ä¾¿è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    memory=ConversationBufferMemory() # ä½¿ç”¨ConversationBufferMemory ä½œä¸ºè®°å¿†
)

conversation.predict(input="Hi there!")

```

    " Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?"


```python
conversation.predict(input="I'm doing well! Just having a conversation with an AI.")
```

    " That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?"


```python
conversation.predict(input="Tell me about yourself.")
```

    ' Of course! As I mentioned, I am an AI created by OpenAI. I am designed to process and analyze large amounts of data, and use that information to perform various tasks and provide helpful responses. I am constantly learning and adapting to new information, which allows me to improve my abilities over time. Is there anything else you would like to know about me?'

### 4.2ã€SequentialChain

![](./img/sec.png)

é¡ºåºé“¾å¯ä»¥ç»„åˆå¤šä¸ªchainï¼ŒSequentialChainå‚æ•°è¾“å…¥chainåˆ—è¡¨ï¼Œå®ƒä¼šé¡ºåºæ‰§è¡Œæ¯ä¸€ä¸ª
chainï¼Œå°†ç¬¬ä¸€ä¸ªchainçš„è¿”å›å€¼è¾“å…¥åˆ°ç¬¬äºŒchainï¼Œä¾æ¬¡ç±»æ¨ã€‚ä¸‹é¢æ˜¯ä½¿ç”¨èŒƒä¾‹:


```python
from langchain_openai import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import SimpleSequentialChain

# å®šä¹‰ç¬¬ä¸€ä¸ªchain
llm = OpenAI(temperature=.7)
template = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
Title: {title}
Playwright: This is a synopsis for the above play:"""
prompt_template = PromptTemplate(input_variables=["title"], template=template)
synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)

# å®šä¹‰ç¬¬äºŒä¸ªchain
llm = OpenAI(temperature=.7)
template = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
Play Synopsis:
{synopsis}
Review from a New York Times play critic of the above play:"""
prompt_template = PromptTemplate(input_variables=["synopsis"], template=template)
review_chain = LLMChain(llm=llm, prompt=prompt_template)

# é€šè¿‡ç®€å•é¡ºåºé“¾ç»„åˆä¸¤ä¸ªLLMChain
overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)
# æ‰§è¡Œé¡ºåºé“¾
review = overall_chain.invoke("Tragedy at sunset on the beach")
```

    Tragedy at Sunset on the Beach follows the story of two young lovers, Mia and Jack....


### 4.3ã€RouterChain

![](./img/rc.png)

RouterChainæ˜¯ä¸€ç§ç¤ºä¾‹ï¼Œç”¨äºåˆ›å»ºåŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªé“¾ï¼Œä»¥å¤„ç†ç»™å®šçš„è¾“å…¥ã€‚RouterChain
é€šå¸¸ç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼š

- RouterChainæœ¬èº«ï¼ˆè´Ÿè´£é€‰æ‹©è¦è°ƒç”¨çš„ä¸‹ä¸€æ¡é“¾ï¼‰ã€‚
- destination_chains: è·¯ç”±å™¨é“¾å¯ä»¥è·¯ç”±åˆ°çš„é“¾ã€‚

ä¸‹é¢å°†å±•ç¤ºè¿™äº›è·¯ç”±é“¾åœ¨MultiPromptChainä¸­çš„åº”ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªé—®é¢˜å›ç­”é“¾æ¡ï¼Œæ ¹æ®ç»™å®š
çš„é—®é¢˜é€‰æ‹©æœ€ç›¸å…³çš„æç¤ºï¼Œç„¶åä½¿ç”¨è¯¥æç¤ºå›ç­”é—®é¢˜ã€‚

```python
from langchain.chains.router import MultiPromptChain
from langchain_openai import OpenAI
from langchain.chains import ConversationChain
from langchain.chains.llm import LLMChain
from langchain.prompts import PromptTemplate

physics_template = """You are a very smart physics professor. \
You are great at answering questions about physics in a concise and
easy to understand manner. \
When you don't know the answer to a question you admit that you
don't know.
Here is a question:
{input}"""

math_template = """You are a very good mathematician. You are great
at answering math questions. \
You are so good because you are able to break down hard problems
into their component parts, \
answer the component parts, and then put them together to answer
the broader question.
Here is a question:
{input}"""

prompt_infos = [
    {
        "name": "physics",
        "description": "Good for answering questions about physics",
        "prompt_template": physics_template
    },
    {
        "name": "math",
        "description": "Good for answering math questions",
        "prompt_template": math_template
    }
]

llm = OpenAI()
destination_chains = {}

# å°†prompt_infosåˆ—è¡¨ä¸­çš„æ¯ä¸ªpromptä¿¡æ¯è½¬æ¢ä¸ºä¸€ä¸ªConversationChainå®ä¾‹
for p_info in prompt_infos:
    name = p_info["name"]
    prompt_template = p_info["prompt_template"]
    prompt = PromptTemplate(template=prompt_template, input_variables=["input"])
    chain = LLMChain(llm=llm, prompt=prompt)
    destination_chains[name] = chain

# LLMRouterChainé“¾æ¡ä½¿ç”¨ä¸€ä¸ªLLMæ¥ç¡®å®šå¦‚ä½•è¿›è¡Œè·¯ç”±ã€‚
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

destinations = [f"{p['name']}: {p['description']}" for p in prompt_infos]
destinations_str = "\n".join(destinations)
# physics: Good for answering questions about physics
# math: Good for answering math questions

router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations=destinations_str
)
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)

default_chain = ConversationChain(llm=llm, output_key="text")
router_chain = LLMRouterChain.from_llm(llm, router_prompt)
chain = MultiPromptChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True)
print(chain.invoke("What is black body radiation?"))
```


    physics: {'input': 'What is black body radiation?'}
    
    Black body radiation is the electromagnetic radiation emitted by ...

```python
print(chain.invoke("What is the first prime number greater than 40 such that one plus the prime number is divisible by 3"))
```


    math: {'input': 'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3'}
    
    The first prime number greater than 40 that satisfies this ...

```python
print(chain.invoke("What is the name of the type of cloud that rins"))
```


    None: {'input': 'What is the name of the type of cloud that rins'}
    
    There are various types of clouds in computing, such as public cloud...


### 4.4ã€å…¶ä»–Chains

LangChainè¿˜å†…ç½®äº†ä¸€äº›ä¸“é—¨ç”¨äºç‰¹å®šåŠŸèƒ½çš„é“¾ï¼Œæ ¹æ®æ•°æ®å¤„ç†çš„ä¸åŒå½¢å¼å®šä¹‰äº†ä¸åŒçš„é“¾
æ¡ç±»å‹:

- Stuff Documents Chainï¼šè¿™æ˜¯æœ€ç›´æ¥çš„æ–‡æ¡£é“¾ã€‚å®ƒæ¥æ”¶ä¸€ç³»åˆ—æ–‡æ¡£ï¼Œå°†å®ƒä»¬å…¨éƒ¨
æ’å…¥åˆ°ä¸€ä¸ªæç¤ºä¸­ï¼Œç„¶åå°†è¯¥æç¤ºä¼ é€’ç»™ä¸€ä¸ªLLMï¼ˆLanguage Modelï¼‰ã€‚è¿™ä¸ªé“¾
é€‚åˆäºæ–‡æ¡£è¾ƒå°ä¸”å¤§å¤šæ•°è°ƒç”¨åªä¼ å…¥å°‘é‡æ–‡æ¡£çš„åº”ç”¨ã€‚
- Refine Documents Chainï¼šè¿™ä¸ªé“¾é€šè¿‡å¾ªç¯å¤„ç†è¾“å…¥æ–‡æ¡£å¹¶è¿­ä»£æ›´æ–°å…¶ç­”æ¡ˆæ¥æ„
å»ºå“åº”ã€‚å¯¹äºæ¯ä¸ªæ–‡æ¡£ï¼Œå®ƒå°†æ‰€æœ‰éæ–‡æ¡£è¾“å…¥ã€å½“å‰æ–‡æ¡£å’Œæœ€æ–°çš„ä¸­é—´ç­”æ¡ˆä¼ é€’ç»™
ä¸€ä¸ªLLMé“¾ä»¥è·å–æ–°çš„ç­”æ¡ˆã€‚è¿™ä¸ªé“¾é€‚åˆéœ€è¦åˆ†ææ¯”æ¨¡å‹ä¸Šä¸‹æ–‡å¯ä»¥å®¹çº³çš„æ›´å¤šæ–‡
æ¡£çš„ä»»åŠ¡ã€‚
- Map Reduce Documents Chainï¼šè¿™ä¸ªé“¾é¦–å…ˆå•ç‹¬å°†LLMé“¾åº”ç”¨åˆ°æ¯ä¸ªæ–‡æ¡£ä¸Š
ï¼ˆMapæ­¥éª¤ï¼‰ï¼Œå¹¶å°†é“¾çš„è¾“å‡ºä½œä¸ºæ–°çš„æ–‡æ¡£ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå°†æ‰€æœ‰è¿™äº›æ–°æ–‡æ¡£ä¼ é€’ç»™
ä¸€ä¸ªç‹¬ç«‹çš„combine documentsé“¾ï¼Œä»¥è·å–ä¸€ä¸ªå•ä¸€çš„è¾“å‡ºï¼ˆReduceæ­¥éª¤ï¼‰ã€‚å¦‚
æœéœ€è¦ï¼Œå®ƒè¿˜å¯ä»¥åœ¨ä¼ é€’åˆ°combine documentsé“¾ä¹‹å‰å¯¹æ˜ å°„çš„æ–‡æ¡£è¿›è¡Œå‹ç¼©æˆ–æŠ˜
å ï¼Œä»¥ç¡®ä¿å®ƒä»¬é€‚åˆåœ¨è¿™ä¸ªé“¾ä¸­ã€‚
- Map Re-rank Documents Chainï¼šDocuments Chainè¿™ä¸ªé“¾é¦–å…ˆå¯¹æ¯ä¸ªæ–‡æ¡£è¿è¡Œä¸€
ä¸ªåˆå§‹æç¤ºï¼Œè¯¥æç¤ºä¸ä»…è¯•å›¾å®Œæˆä»»åŠ¡ï¼Œè¿˜å¯¹ç­”æ¡ˆçš„ç¡®å®šç¨‹åº¦è¿›è¡Œè¯„åˆ†ã€‚ç„¶åè¿”å›
è¯„åˆ†æœ€é«˜çš„å“åº”ã€‚
- æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰Chainã€‚

