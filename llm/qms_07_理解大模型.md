# ç†è§£å¤§æ¨¡å‹

äººç±»å¾ˆéš¾ç†è§£äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå› ä¸ºå®ƒä»¬åºå¤§ä¸”ä¸é€æ˜ã€‚Transluceçš„ä¸€äº›å·¥å…·å¯ä»¥å¸®åŠ©ç†è§£ AI ç³»ç»Ÿ

## æè¿°ç¥ç»å…ƒ

> refï¼šhttps://transluce.org/neuron-descriptions

å¦‚ä½•æè¿°ä¸€ä¸ªç¥ç»å…ƒçš„è¡Œä¸ºï¼Ÿå…ˆçœ‹å‡ ä¸ªä¾‹å­ï¼Œä¸‹é¢çš„å›¾ç‰‡ä¸­ï¼Œé¢œè‰²è¶Šæ·±ï¼Œä»£è¡¨è¿™ä¸ªç¥ç»å…ƒçš„æ¿€æ´»å€¼è¶Šå¤§

![](./img/lj1.jpg)

è§‚å¯Ÿï¼Œæ€»ç»“è§„å¾‹ï¼Œæ¨¡å‹å’Œäººç±»ç»™å‡ºçš„æè¿°åˆ†åˆ«æ˜¯ï¼š

```
Model: Technical terms related to reinforcement learning (e.g., "off-policy", "learning", "confidence rate"). (score: 0.83)

Human: Terms related to reinforcement learning, such as efficiency, RL, policy, learning, converge, training, and outcome measures. (score: 0.74)
```

------

![](./img/lj2.jpg)

```
Model: Specific multi-character tokens or character repetitions, particularly in phrases and numbers (e.g., "é¡¶{é¡¶é¡¶}", "10,{10}", "F151{5}T", "æˆ‘æƒ³åƒé¡¶{é¡¶}"). (score: 0.71)

Human: Repeated instances of a word or number after the first time it appears, like cats {cats cats}, or 1-{1}0 (score: 0.70)
```

ä»»åŠ¡å®šä¹‰ï¼š

- ç»™å®šæ¨¡å‹ï¼Œæ¨¡å‹ä»åŸŸ ğ· ä¸­è·å–è¾“å…¥ã€‚å¯¹äºä¸€ä¸ª feature $$\phi:D\to\mathbb{R}$$ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªè‡ªç„¶è¯­è¨€æè¿° hï¼Œæè¿° ğœ™ çš„è¡Œä¸º
- ä¼šé€šè¿‡ä¸€ä¸ªæ¨¡æ‹Ÿå™¨æ¨¡å‹ $$\sigma(d|h)$$ æ¥å½¢å¼åŒ–è¿™ä¸€ç‚¹ï¼Œå®ƒåœ¨ç»™å®šè¾“å…¥ $$d\in D$$ å’Œæè¿° h çš„æƒ…å†µä¸‹é¢„æµ‹æ¿€æ´»å€¼
- ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªè§£é‡Šå™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ºç»™å®šç‰¹å¾ ğœ™ ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿° h

![](./img/automated_desc_gen_overview.png)

æ›´è¯¦ç»†åœ°è¯´ï¼Œä¸Šé¢æè¿°çš„æ–¹æ³•æœ‰å‡ ä¸ªæ­¥éª¤ï¼š

1. ç”Ÿæˆç¤ºä¾‹ï¼šæ„å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–è¾“å…¥çš„æ¨¡å‹è¯­æ–™åº“ ğ· ï¼Œå¹¶ä½¿ç”¨å…¶ä¸­ ğœ™(ğ‘‘)  æœ€å¤§çš„ m ä¸ªè¾“å…¥ä½œä¸ºç»™å®šç‰¹å¾ ğœ™ çš„ç¤ºä¾‹
2. æè¿°ç¤ºä¾‹ï¼šé€šè¿‡æç¤ºæˆ–è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥ç”ŸæˆåŸºäºè¿™äº›ç¤ºä¾‹çš„ ğœ™ æè¿° h
3. è¯„åˆ†æè¿°ï¼šæ ¹æ®æè¿° h åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹ ğœ™ è¡Œä¸ºçš„å‡†ç¡®æ€§ï¼Œè¿›è¡Œè¯„åˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ ğœï¼Œå®ƒé¢„æµ‹æ¿€æ´»å€¼ 
4. è’¸é¦ï¼šæœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€é«˜å¾—åˆ†çš„æè¿°æ¥è’¸é¦ä¸€ä¸ªæ–°æ¨¡å‹

ä¸€äº›æœ‰è¶£çš„ç»“æœï¼šæ­£ææ€§å’Œè´Ÿææ€§çš„æ¿€æ´»æ¨¡å¼é€šå¸¸æ˜¯ç›¸å…³çš„ï¼Œè¿™å’Œæˆ‘ä»¬åœ¨[è¯åµŒå…¥](https://qmmms.gitbook.io/note/deep_learning/qms11-zi-ran-yu-yan-chu-li-ren-wu#te-xing-yu-ying-yong)å­¦åˆ°çš„ç±»ä¼¼ï¼š

ä¾‹å­1ï¼š**West/East coast**

![](./img/lj3.jpg)

ä¾‹å­2ï¼š**Bird/Bug**

![](./img/lj4.jpg)

åœ¨æœ‰æ¨¡å‹å¸®åŠ©æˆ‘ä»¬åšè¿™äº›äº‹æƒ…ä¹‹åï¼Œå°±å¯ä»¥å¿«é€Ÿæ„å»ºä¸€ä¸ªæ•°æ®åº“ï¼ŒåŒ…å«ä¸€ä¸ªå¤§æ¨¡å‹æ‰€æœ‰ç¥ç»å…ƒçš„æè¿°ï¼Œå¯ä»¥ç”¨äºåç»­æµç¨‹ä¸­

## å¤§æ¨¡å‹è¶Šç‹±ä¸è¯±å¯¼

> ref: https://transluce.org/automated-elicitation

## 9.11å’Œ9.9å“ªä¸ªå¤§ï¼Ÿ

> ref: https://monitor.transluce.org/dashboard/chat

Transluceçš„å·¥å…·Monitor æä¾›äº†å¿«æ·çš„æ–¹å¼å¸®åŠ©æˆ‘ä»¬åˆ†æå…¶é”™è¯¯ã€‚å°†å…‰æ ‡æ”¾åœ¨å‡ºé”™çš„ä½ç½®ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹åœ¨æ­¤å¤„é¢„æµ‹çš„è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚

![](./img/lj5.jpg)

ç‚¹å‡»ä¸€ä¸‹è¿™ä¸ªé”™è¯¯ï¼ŒMonitor å¼€å§‹åˆ†ææ¨¡å‹å‡ºé”™çš„å¯èƒ½åŸå› ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¼šã€Œå¯»æ‰¾å½±å“ bigger è¿™ä¸ªé¢„æµ‹ç»“æœçš„ç¥ç»å…ƒã€ã€‚ä¹‹åï¼ŒMonitor ä¼šå¯¹è¿™äº›ç¥ç»å…ƒè¿›è¡Œèšç±»ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œæ­¤å¤„æœ‰ 4 ä¸ªèšç±»ï¼š

![](./img/lj6.jpg)

ç²—ç•¥æ¥çœ‹ï¼ŒLlama 3.1 8B åœ¨çœ‹åˆ° 9.11 å’Œ 9.9 è¿™ä¸¤ä¸ªæ•°å­—æ–‡æœ¬æ—¶ï¼Œé¦–å…ˆæƒ³åˆ°çš„å¹¶ä¸æ˜¯å•çº¯çš„æ•°å€¼ï¼Œè€Œæ˜¯ä¼šå’Œäººç±»ä¸€æ ·è”æƒ³åˆ°ç›¸å…³çš„å…¶å®ƒæ¦‚å¿µï¼Œæ¯”å¦‚ 9/11 è¢­å‡»å’Œä¹‹åçš„ææ€–è¢­å‡»ã€ã€Šåœ£ç»ã€‹ç« èŠ‚å’Œè¯—æ–‡ç¼–å·ã€åŒ–å­¦åŒ–åˆç‰©å’Œåˆ†å­å¼ã€æ—¥æœŸç­‰ç­‰ã€‚å¹¶ä¸”å…¶ä¸­æ¯ä¸€ç§ã€Œè”æƒ³ã€éƒ½ä¼šè§¦å‘ä¸åŒçš„ç¥ç»å…ƒç»„åˆã€‚

![](./img/lj8.gif)

é€‰æ‹©å…¶ä¸­ä¸€ä¸ªå±•å¼€ï¼Œå¯ä»¥çœ‹åˆ°å½±å“ AI æ¨¡å‹åšå‡ºã€Œbiggerã€è¿™ä¸ªåˆ¤æ–­çš„ç¥ç»å…ƒè¯¦æƒ…ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ¯ä¸ªç¥ç»å…ƒçš„æè¿°ï¼Œæ¥è‡ªä¸Šé¢è¯´åˆ°çš„è‡ªåŠ¨ç”Ÿæˆæè¿°çš„æ¨¡å‹

æˆ‘ä»¬å¯ä»¥ç‚¹å¼€ä¸€ä¸ªå…·ä½“çš„ç¥ç»å…ƒæŸ¥çœ‹ï¼Œæ¯”å¦‚ç¬¬ 2 å±‚çš„ 1054 å·ç¥ç»å…ƒã€‚è¿™é‡Œå±•ç¤ºäº†å…¶åœ¨æ¥æ”¶æç¤ºè¯ä¹‹åçš„æ­£å€¼æ¿€æ´»æƒ…å†µ

é€šè¿‡åˆ†æè¿™äº›ç¥ç»å…ƒï¼Œæˆ‘ä»¬å¯ä»¥æ´è§æ¨¡å‹å‡ºé”™çš„æ ¹æœ¬åŸå› ï¼šæ¨¡å‹æ ¹æœ¬æ²¡æŠŠ 9.11 å½“æˆæ•°å€¼ï¼Œè€Œæ˜¯çœ‹æˆäº†ä¸€ä¸ªæ—¥æœŸï¼Œè¿™æ ·è¿å¸¦ä¸‹æ¥ï¼Œ9.9 è‡ªç„¶ä¹Ÿæ˜¯ä¸€ä¸ªæ—¥æœŸäº†ã€‚äºæ˜¯ï¼Œ9 æœˆ 11 å·è‡ªç„¶å°±æ¯” 9 æœˆ 9 æ—¥ biggerã€‚å¦å¤–ï¼Œåœ¨ã€Šåœ£ç»ã€‹ä¸­ï¼Œ9.11 ä¹Ÿæ˜¯æ¯” 9.8 æ›´é åçš„ç¼–å·ã€‚è€Œä¸ç®¡æ˜¯ 9/11 äº‹ä»¶è¿˜æ˜¯ã€Šåœ£ç»ã€‹ï¼Œæ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸­éƒ½åŒ…å«å¤§é‡ç›¸å…³çš„æ–‡æœ¬å†…å®¹ï¼Œè¿™ä¼šå½±å“åˆ°æ¨¡å‹åœ¨åˆ¤æ–­è¿™ä¸ªæ•°å­—æ—¶çš„ç¥ç»å…ƒæ¿€æ´»æƒé‡ã€‚

Monitor è¿˜æä¾›äº†è¿›ä¸€æ­¥çš„æ£€æŸ¥æŠ€æœ¯ï¼Œå¯ä»¥é€šè¿‡å°†ç›¸åº”æ¿€æ´»å¼ºè¡Œè®¾ç½®ä¸º 0 æ¥ä¿®æ­£ AI æ¨¡å‹çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¯ä»¥

- å°†å…³è”ã€Šåœ£ç»ã€‹ç« èŠ‚ç¼–å·çš„ç¥ç»å…ƒæ¿€æ´»è°ƒæˆ 0ï¼Œå°±æ˜¯é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°ç¥ç»å…ƒæè¿°ä¸â€œåœ£ç»â€ç›¸å…³çš„500ä¸ªï¼Œæ¿€æ´»å€¼è®¾ç½®ä¸º0
- å°†å¯¹åº”ã€Œæ—¥æœŸã€çš„ç›¸å…³ç¥ç»å…ƒçš„æ¿€æ´»æ”¹æˆ 0 

æ¨¡å‹å¯ä»¥æ­£ç¡®è¾“å‡º 9.9 is bigger than 9.11ï¼Œè€Œä¿®å¤è¿™ä¸ªé—®é¢˜çš„ä»£ä»·ä»…ä»…æ˜¯æŠ‘åˆ¶äº†ä¸åˆ° 0.2% çš„ MLP ç¥ç»å…ƒã€‚

é™¤äº†æ¯”è¾ƒ 9.11 å’Œ 9.9 çš„å¤§å°ï¼Œå®˜æ–¹è¿˜æä¾›äº†å¦å¤–ä¸‰ä¸ªç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä¿®å¤ AI éš¾ä»¥æ•°å€¼æ’åºçš„é—®é¢˜ã€å¼•å¯¼å‡ºéšè—çŸ¥è¯†ã€å¼•å¯¼æ•…äº‹ä¸­ç‰¹å®šè§’è‰²ã€‚å…¶ä¸­çš„æ“ä½œä¸ä»…åŒ…æ‹¬å°†æ¿€æ´»æ¸…é›¶ï¼Œä¹ŸåŒ…æ‹¬å¢å¼ºæŸäº›ç‰¹å®šç¥ç»å…ƒä»¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆç”¨æˆ·éœ€æ±‚çš„ç»“æœ

ä¾‹å¦‚ï¼Œå¦‚æœè¦è§£å†³å¦ä¸€ä¸ªé—®é¢˜ï¼šStrawberry ä¸­æœ‰å‡ ä¸ª rï¼Œæ–¹æ¡ˆå¦‚ä¸‹ï¼š

1. æ ¹æ® Monitor åˆ†æï¼ŒLlama 3.1 8B æ¨¡å‹åœ¨å›ç­”è¿™ä¸ªé—®é¢˜æ—¶ä¼šå°† Strawberry æ‹†åˆ†æˆä¸¤éƒ¨åˆ†ï¼šStraw å’Œ berryï¼ŒåŒæ—¶ Strawberry è¿˜æ¿€æ´»äº†ä¸é£Ÿå“å’Œä½æ–™ç›¸å…³çš„ç¥ç»å…ƒ
2. æŠ‘åˆ¶ Monitor æ‰¾åˆ°çš„æ‰€æœ‰ç¥ç»å…ƒæ¿€æ´»
3. ä»¥ã€ŒStrawberry as a string made of several English lettersï¼ˆå°† Strawberry çœ‹ä½œæ˜¯ä¸€ä¸ªç”±è‹±è¯­å­—æ¯æ„æˆçš„å­—ç¬¦ä¸²ï¼‰ã€ä½œä¸ºæœç´¢æ¡ä»¶ï¼Œå®šä½åˆ° 50 ä¸ªç›¸å…³ç¥ç»å…ƒï¼Œå…¨éƒ¨å¢å¼ºå®ƒä»¬
4. Llama 3.1 8B ç»™å‡ºäº†æ­£ç¡®ç­”æ¡ˆ
5. åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºè¿™æ ·çš„æ´è§ï¼šåœ¨è§£ç­”ã€ŒStrawberry ä¸­æœ‰å‡ ä¸ª rã€è¿™æ ·çš„é—®é¢˜æ—¶ï¼ŒLLM çš„é—®é¢˜æ˜¯æƒ³å¾—å¤ªå¤šï¼Œå»åˆ†æå…¶èƒŒåæ‰€ä»£è¡¨çš„æ„ä¹‰å’Œäº‹ç‰©äº†ï¼Œè€Œå®ƒåŸæœ¬åªéœ€è¦å°†å…¶çœ‹æˆä¸€ä¸ªå­—ç¬¦ä¸²å³å¯

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªç³»ç»ŸèƒŒåçš„æŠ€æœ¯

![](./img/lj7.jpg)

å¼€å‘è€…é‡‡ç”¨ä¸€ä¸ªé¢„å…ˆç¼–è¯‘å¥½çš„ç¥ç»å…ƒæè¿°æ•°æ®åº“ï¼ˆæ¥è‡ªæè¿°ç¥ç»å…ƒä¸€ç« ï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªæè¿°éƒ½ä¸æœ€èƒ½æ¿€æ´»è¯¥ç¥ç»å…ƒçš„ K ä¸ªå…³é”®ç¤ºä¾‹å’Œå®ƒä»¬çš„æ¿€æ´»æ¨¡å¼ç›¸è”ç³»

è¯¥å›¢é˜Ÿé‡‡ç”¨äº† OpenAI çš„ text-embedding-3-large åµŒå…¥æŠ€æœ¯æ¥å¤„ç†è¿™äº›æè¿°ï¼Œåˆ›å»ºä¸€ä¸ªç”¨äºè¯­ä¹‰æœç´¢çš„ç´¢å¼•å‘é‡æ•°æ®åº“ï¼ˆVectorDBï¼‰

ä½¿ç”¨ä¸€ä¸ª AI linter æ¥çªå‡ºæ˜¾ç¤ºç›¸å…³çš„ç¥ç»å…ƒç°‡ã€‚é¦–å…ˆï¼Œä»–ä»¬è®© GPT-4o mini ç®€åŒ–å¹¶æ¦‚æ‹¬ç¥ç»å…ƒçš„æè¿°ã€‚ç„¶åï¼Œä»–ä»¬ä½¿ç”¨ OpenAI çš„åµŒå…¥æŠ€æœ¯ï¼ˆtext-embedding-3-largeï¼‰æ¥åµŒå…¥ç¥ç»å…ƒï¼Œå¹¶ä½¿ç”¨å±‚æ¬¡èšç±»æ–¹æ³•ï¼Œæ ¹æ®ä½™å¼¦ç›¸ä¼¼åº¦å°†ç¥ç»å…ƒèšç±»ï¼Œä½¿ç”¨ 0.6 çš„é˜ˆå€¼ã€‚æœ€åï¼Œä»–ä»¬è®© GPT-4o mini ä¸ºæŸç°‡ç®€åŒ–è¿‡çš„ç¥ç»å…ƒå†ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æè¿°ï¼Œå¹¶æ ¹æ®ç°‡å†…ç¥ç»å…ƒåœ¨è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼åº¦æ‰“ä¸€ä¸ªåˆ†æ•°ï¼ˆ1-7ï¼‰ï¼Œå…¶ä¸­ 1 ä»£è¡¨æœ€ç›¸ä¼¼ã€‚åœ¨ Monitor çš„ç•Œé¢ä¸­ï¼Œåªæ˜¾ç¤ºæ•°é‡å¤§äºç­‰äºä¸‰ä¸ªï¼Œä¸”å¾—åˆ†å°äºç­‰äº 3 çš„ç°‡

å¼€å‘è€…é€šè¿‡å°†ç¥ç»å…ƒçš„æ¿€æ´»å€¼å›ºå®šåœ¨æŒ‡å®šå€¼æ¥å¼•å¯¼å®ƒä»¬ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœéœ€è¦åœ¨ token T ä¸Šå¼•å¯¼ä¸€ç»„ç¥ç»å…ƒ Sï¼Œä»¥å¼ºåº¦ Î» è¿›è¡Œæ“ä½œï¼Œåœ¨æ¯ä¸ª token t å±äº T çš„æƒ…å†µä¸‹ï¼Œåœ¨è¯¥æ ‡è®°çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå°†ç¥ç»å…ƒçš„æ¿€æ´»å€¼è®¾ç½®ä¸ºï¼š$$\lambda \cdot 10^{âˆ’5}$$ã€‚è¿™ä¸ªè¿‡ç¨‹ä¹Ÿä¼šè¿›å…¥è¯¥å±‚çš„æ®‹å·®ä¸­ï¼Œè¿›è€Œå½±å“åç»­çš„å±‚ä¸æ³¨æ„åŠ›å¤´ã€‚è¿™ä¸€æ“ä½œä¼šè¦†ç›–æ‰€æœ‰éœ€è¦å¼•å¯¼çš„ç¥ç»å…ƒ sã€‚ç”±äºç¥ç»å…ƒå…·å¤‡æ­£è´Ÿä¸¤ç§ææ€§ï¼Œå½“æˆ‘ä»¬æŒ‡å®šä¸€ä¸ªç¥ç»å…ƒæ—¶ï¼Œåªæœ‰å½“å®ƒä¸å¼•å¯¼é›†ä¸­æŒ‡å®šçš„ææ€§ç›¸åŒæ—¶æ‰ä¼šç”Ÿæ•ˆã€‚

å¦‚ä½•è¡¡é‡å“ªä¸ªç¥ç»å…ƒåœ¨ç‰¹å®šä»»åŠ¡ä¸­æ›´æ´»è·ƒï¼Ÿæä¾›äº†æ¿€æ´»å’Œå½’å› ä¸¤ç§æ¨¡å¼ã€‚æ¿€æ´»ä¸»è¦å…³æ³¨ç¥ç»å…ƒçš„åŸå§‹æ¿€æ´»å€¼ï¼Œå³ä¸Šä¸€èŠ‚ä¸­çš„ Î»ï¼Œå¦‚æœè¿™ä¸ªå€¼è¿œé«˜äºå¹³å‡å€¼ï¼Œé‚£ä¹ˆå®ƒå¾ˆå¯èƒ½åœ¨ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚ å½’å› æ˜¯ä¸€ç§æ›´å…·é’ˆå¯¹æ€§çš„æ¨¡å¼ï¼Œå®ƒæµ‹é‡ç¥ç»å…ƒå¯¹ç‰¹å®šè¾“å‡º token çš„å½±å“ã€‚å— Attribution Patching å¯å‘ï¼Œè®¡ç®—è¾“å‡º token çš„å¯¹æ•°æ¦‚ç‡ z ç›¸å¯¹äºç¥ç»å…ƒæ¿€æ´»å€¼ e çš„æ¢¯åº¦ï¼Œå½’å› å€¼ç­‰äº $$e \frac{\partial z}{\partial e}$$

åœ¨ Monitor ä¸­ï¼Œç³»ç»Ÿå°†ä¼šå¯»æ‰¾ä¸ç”¨æˆ·æœç´¢æŸ¥è¯¢æœ€ä¸ºåŒ¹é…çš„ k ä¸ªç¥ç»å…ƒã€‚è¿™äº›ç¥ç»å…ƒä¼šè¢«é€‰ä¸ºå¼•å¯¼é›†ï¼Œå¸®åŠ©æˆ‘ä»¬å†³å®šå“ªäº›ç‰¹å¾éœ€è¦è¢«å‡å¼±ï¼ˆåœç”¨ï¼‰æˆ–è€…åŠ å¼ºï¼ˆå¢å¼ºï¼‰ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å‡å¼±ä¸€ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬ä¼šè®¾ç½®å¼•å¯¼å€¼ä¸º 0ï¼ˆÎ»=0ï¼‰ï¼›å¦‚æœæˆ‘ä»¬æƒ³è¦åŠ å¼ºä¸€ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬ä¼šè®¾ç½®å¼•å¯¼å€¼ä¸º 0.5ï¼ˆÎ»=0.5ï¼‰

ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦ï¼Œé€šè¿‡ç‚¹å‡»é«˜çº§é€‰é¡¹æ¥è°ƒæ•´ k çš„æ•°å€¼ï¼ˆä¹Ÿå°±æ˜¯ä»–ä»¬æƒ³è¦å½±å“çš„ç¥ç»å…ƒæ•°é‡ï¼‰å’Œ Î» çš„æ•°å€¼ï¼ˆä¹Ÿå°±æ˜¯å½±å“çš„ç¨‹åº¦ï¼‰ã€‚ä»–ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è¢«é€‰ä¸ºå¼•å¯¼é›†çš„ç¥ç»å…ƒç¤ºä¾‹ï¼Œå¹¶å†³å®šä»–ä»¬æƒ³è¦å½±å“çš„ç‰¹å®šçš„ token å­é›†ã€‚å¦‚æœæ²¡æœ‰ç‰¹åˆ«æŒ‡å®šï¼Œç³»ç»Ÿé»˜è®¤ä¼šå½±å“åˆå§‹ç³»ç»Ÿå’Œç”¨æˆ·æç¤ºä¸­çš„æ‰€æœ‰ token

## LLM çš„è¯è¡¨åº”è¯¥é€‰å¤šå¤§?

ä¸€èˆ¬è€Œè¨€ï¼Œè¯è¡¨å˜å¤§ä¼šæ›´å¥½

- ç›¸åŒçš„æ–‡æœ¬ï¼Œè½¬æ¢ä¸ºtokenåè¶ŠçŸ­è¶Šå¥½ã€‚é€šå¸¸ç”¨å‹ç¼©ç‡æ¥è¡¡é‡æ–‡æœ¬è½¬æ¢ä¸ºtokenåçš„å‹ç¼©æ¯”ä¾‹ã€‚æ›´é«˜çš„å‹ç¼©ç‡ä»£è¡¨äº†ç›¸åŒæ•°é‡çš„tokenèƒ½å¤Ÿè¡¨è¾¾æ›´å¤šçš„ä¿¡æ¯ï¼Œç›¸åŒçš„ä¿¡æ¯ token è¶ŠçŸ­åˆ™è®­ç»ƒæ•ˆç‡æ›´é«˜
- æ›´å¤šçš„è¯æ±‡èƒ½å¤Ÿå‡å°‘ OOV (Out of Vocabulary)çš„å½±å“, è®­ç»ƒçš„ä¿¡æ¯ä¸ä¼šä¸¢å¤±ï¼Œæ¨ç†çš„æ—¶å€™æ³›åŒ–èƒ½åŠ›ä¹Ÿæ›´å¼ºã€‚åŒæ—¶æ›´å¤šçš„è¯æ±‡å¯ä»¥å‡å°‘è¯æ±‡åˆ†è§£åçš„æ­§ä¹‰ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆæ–‡æœ¬
- é¢„è®­ç»ƒé˜¶æ®µå¾€å¾€éƒ½æœ‰æœ€å¤§åºåˆ—é•¿åº¦çš„é™åˆ¶ï¼Œ**å‹ç¼©ç‡æ›´é«˜ä»£è¡¨ç€èƒ½çœ‹åˆ°æ›´å¤šçš„ä¸Šä¸‹æ–‡**ï¼Œå°±èƒ½ attention åˆ°æ›´å¤šçš„ä¿¡æ¯

ä»è®¡ç®—æ•ˆç‡ä¸Šè€ƒè™‘

- vocabularyä¸èƒ½æ— é™æ‰©å¤§ã€‚vocabulary å˜å¤§åï¼ŒEmbedding å±‚å˜å¤§ï¼Œæœ€åè¾“å‡ºçš„ Head layer ä¹Ÿä¼šå˜å¤§
- å‚æ•°æ›´å¤§ï¼Œæ›´å å†…å­˜ï¼Œè€Œä¸”è¾“å‡ºçš„æ—¶å€™ softmax ä¹Ÿæ›´å¤§ï¼Œè®¡ç®—å°±æ›´æ…¢ã€‚è™½ç„¶å¤§å¤šæ•°æƒ…å†µä¸‹ token é‡çš„å‡å°‘ï¼Œæ•´ä½“ä¸Šæ˜¯ç®—å¾—æ›´å¿«çš„
- ç›®å‰**ä¸šç•Œæ™®éè®¾ç½®åœ¨ 10ä¸‡ åˆ° 20ä¸‡å·¦å³**ã€‚æ¯”å¦‚ Qwen çš„ è¯è¡¨å¤§å°ä¸º 152064ï¼Œbaichuan2ä¸º125696ï¼Œllama3 ä¸º128256ï¼Œdeepseek ä¸º 102400ã€‚å¤šæ¨¡æ€çš„ä¼šæ›´å¤§ä¸€äº›
- è¦è€ƒè™‘å†…å­˜å¯¹é½ã€‚vocabulary çš„**å¤§å°è®¾ç½®è¦æ˜¯ 8 çš„å€æ•°ï¼Œåœ¨ A100 ä¸Šåˆ™æ˜¯ 64 çš„å€æ•°**

> å½“ç„¶ softmax è¿‡å¤§çš„é—®é¢˜ç›®å‰ä¹Ÿæœ‰è§£æ³•ï¼Œå¯ä»¥ç”¨ Adaptive softmaxï¼Œå‚è€ƒè®ºæ–‡ã€ŠEfficient softmax approximation for GPUsã€‹

## å¦‚ä½•è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡ï¼Ÿ

ä¸€äº›å¸¸è§çš„ç½‘ç»œç»“æ„åˆ—ä¸¾å¦‚ä¸‹ï¼š

- **Linear**ï¼šå°±æ˜¯ä¸ªçŸ©é˜µï¼Œæœ‰æ—¶å€™ä¼šåŠ ä¸Šä¸ª biasã€‚ä¸€ä¸ª `Linear(in_features=w, out_features=h, bias=True) `çš„å‚æ•°é‡ä¸ºï¼š`w * h + h`, å¦‚æœ bias = False, åˆ™ ä¸º `w * h`
- **Embedding**ï¼šå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªæ²¡æœ‰ bias çš„ Linearã€‚
- **Norm**
  - Layer Norm é‡Œé¢æœ‰ä¸¤ä¸ªå¯è®­ç»ƒå‚æ•° $$\gamma$$ å’Œ $$\beta$$, å‡è®¾hidden_size çš„å¤§å°ä¸º h, hidden_size çš„æ¯ä¸€ç»´éƒ½æœ‰ä¸¤ä¸ªï¼Œæ‰€ä»¥æ˜¯  `2h` ä¸ª
  - RMSNorm æ¯ä¸€ç»´åˆ™åªæœ‰ä¸€ä¸ªå¯è®­ç»ƒå‚æ•° $$\gamma$$  , æ‰€ä»¥æœ‰ `h` ä¸ª
- **Active** å’Œ **Dropout** æ²¡æœ‰å¯è®­ç»ƒå‚æ•°

ä»¥meta-llama/Meta-Llama-3-8Bä¸¾ä¾‹ï¼š

```python
fromÂ transformersÂ importÂ AutoModelForCausalLM
model_llama3Â =Â AutoModelForCausalLM.from_pretrained(
Â Â Â Â model_path,Â tie_word_embeddings=False,Â 
Â Â Â Â token=access_token)
print(model_llama3)
```

```python
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
```

æ‰€ä»¥ llama3 8B çš„çœŸå®å‚æ•°é‡ä¸ºï¼š128256 * 4096 + 32 * (4096 * 4096 * 2 + 4096 * 1024 * 2 + 4096 * 14336 * 3 + 2 * 4096) + 4096 + 128256 * 4096 = 8030261248

ä»¥Qwen 2 7Bä¸¾ä¾‹ï¼š

```python
model_qwen_lmÂ =Â AutoModelForCausalLM.from_pretrained(
Â Â Â Â model_qwen,Â trust_remote_code=True)
print(model_qwen_lm)
```

```python
QWEnLMHeadModel(
  (transformer): QWEnModel(
    (wte): Embedding(151936, 4096)
    (drop): Dropout(p=0.0, inplace=False)
    (rotary_emb): RotaryEmbedding()
    (h): ModuleList(
      (0-31): 32 x QWEnBlock(
        (ln_1): RMSNorm()
        (attn): QWEnAttention(
          (c_attn): Linear(in_features=4096, out_features=12288, bias=True)
          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (core_attention_flash): FlashSelfAttention()
          (attn_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): RMSNorm()
        (mlp): QWEnMLP(
          (w1): Linear(in_features=4096, out_features=11008, bias=False)
          (w2): Linear(in_features=4096, out_features=11008, bias=False)
          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)
        )
      )
    )
    (ln_f): RMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)
)
```

æ‰€ä»¥ qwen 7B çš„çœŸå®å‚æ•°é‡ä¸ºï¼š151936 * 4096  + 32 * (4096 * 2 + 12288 * 4096 + 12288 + 4096 * 4096 + 11008 * 4096 * 3) + 4096 + 4096 * 151936 = 7721324544

> é€šè¿‡å‚æ•°ï¼Œå¯ä»¥çœ‹å‡º llama3 å’Œ qwen2 çš„è¯è¡¨å¤§å°éƒ½æ˜¯64çš„å€æ•°

åˆ©ç”¨ pytorch æä¾›çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„è®¡ç®—å‡ºæ¨¡å‹çš„å‚æ•°é‡ï¼Œåªéœ€è¦ä¸€è¡Œå°±è¡Œï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
defÂ count_parameters(model):
Â Â Â Â returnÂ sum(p.numel()Â forÂ pÂ inÂ model.parameters()Â ifÂ p.requires_grad)
```

