# DeepSeek 笔记

## MoE

MoE（mixture of experts）即混合专家模型，是一种降低训练和推理计算代价的技术。相对的，不使用 MoE 被称为 Dense 模型。

![](./img/moe1.png)

在预训练一个大模型时，我们设定期望损失和模型参数，对于相同的期望损失，如果我们加大模型参数，我们的训练计算花费会变少（不过推理花费会更大）。MoE 模型希望在达到同样期望效果时，训练计算和推理花费都变少。

![](./img/moe2.png)

MoE 主要对传统 decoder only 大模型中 feed forward 层进行了改造，原始的feed forward layer的内部结构很简单，例如先升维到二倍到四倍，在通过线性层变回原来大小

MoE 中，原来的 feed forward 层被拆分为多个小的 feed forward 层，这时每个小的 feed forward layer的第一层相比 Dense 结构的第一层的维度可以大幅减小。例如，从原来的4096乘16384减小为4096乘4096，第二层也一样，以保证输出token的维度还是 4096。 这里被一个小的feed forward layer就被称为一个专家expert。

那对于每个输入的token，应该选择哪个的专家网络呢？这需要一个路由网络来决定。路由网络输出每个token走不同专家网络的概率值，然后给出其中排名靠前的几个（例如两个）专家。token 通过选择的这两个专家网络，得到两个维度为4096的向量，然后再根据路由网络输出的专家权重进行加权求和，得到输出。

![](./img/moe3.png)

> 这里的专家选择是对每个 token 得出的，而不是序列

MoE 的特点是包括

- 在相同的计算代价下，增大了网络参数规模， 从而得到性能更好的大模型
- MOE网络基本可以达到和自己相同参数规模的稠密网络的性能表现
- 相比同等参数规模的稠密网络，MOE网络的计算代价变小了。但是所有专家网络参数都是要加载在显存中的，所以显存占用并没有减少

MoE 网络在训练时比稠密网络要难，因为会有专家在不均衡的问题：大量的token都被少数的几个专家处理，而其他的专家占用了网络参数，但是却不被激活。为了让MOE模型里的专家负载均衡，人们也想了很多办法。包括

- 训练时，每个token至少选择两个专家。如果每次只选择最好的一个专家，只有这个专家可以得到训练，它效果越来越好，权重越来越大，其他专家都得不到训练。所以一般训练时会选择权重最高的专家，同时以其他专家路由的权重作为概率，再随机选择一个专家
- 为了防止某些专家被过度训练，可以给每个专家设置token容量，也就是这个专家在一个batch里能处理的最多的token数，如果超过了这个token数，则对分配给这个专家的token输出为0， 这样这个token就通过残差连接进入下一层。
- 设置一个负载均衡的辅助损失，让模型在训练过程中自己学会负载均衡

我们来详细看一下负载均衡损失，它的目标是希望每个专家被调用的频率是相等的。它的计算公式如下：
$$
f_i = (该专家被调用的次数)/(所有专家给调用的次数)
$$

$$
\mathcal{L}_{\text{balance}}=\sum_i^n(f_i)^2
$$

这里可以通过柯西不等式来严格证明为什么只有负载均衡时，这个loss取最小值。

然而这个损失函数却不能直接拿来利用，因为我们计算每个专家被调用的频率，需要统计每个专家被调用次数，而每个专家是否被调用是通过topk操作进行的。topk操作是根据权重进行的一个选择操作，不是一个数值运算，它是不可微的，无法通过梯度下降进行优化。

一个近似的做法就是把频率的平方的一个频率 $$f_i$$ 用 $$p_i$$ 来代替，它是一个批次中所有token对该专家的路由的概率的平均值。理论上对该专家的路由的平均概率应该等于选择该专家的频率，同时路由的平均概率是通过softmax进行数据计算得到的结果，它是可微的。
$$
\mathcal{L}_{\text{balance}}=\sum_i^n f_i p_i
$$

> 这里虽然还保留了一个 $$f_i$$ ，不过整个式子已经可以被梯度下降优化，因为我们已经为反向传播给出了一个可微的通路，相当于把  $$f_i$$  作为常数

## DeepSeekMoE

DeepSeekMoE 的动机是让专家更加专精。基础的MOE实现中，假设有N个专家，每次选择两个专家，DeepSeek认为传统的MoE设置的专家数太少了，导致每个专家学习了过多的彼此不相关的知识，从而不够专精。

> 比如一个医院里只有两名医生，一个负责内科，一个负责外科，那每个医生需要掌握的医学领域就太广泛了，不利于他医术的进步。

DeepSeek 想到的办法就是将专家进行更进一步的细分，同时每个专家的网络也变小。比如将原来的N个专家变为2N个专家，每个专家网络参数量为原来专家的一半，这样在网络前向传播时，在和之前网络同样的计算代价的前提下，就可以激活四个专家。

> 原来是从8个专家里面选2个，有28种可能的组合；现在是从16个专家里面选4个，有1820种。可以看到对专家进行细分，可以得到更灵活的专家组合。

DeepSeek 又进一步想到，所有的专家可能都要学习一些基础的通用的能力，是否可以将所有专家都要学习的通用技术能力提取出来，作为一个共享专家，保证每次都会激活，他负责所有专家原来都需要的通用能力。然后再从后面(2N-1)专家里面选择3个专家，这样保证专家的总数不变。

> 这里可以理解为医院里面各个科室都需要验血，那就把验血作为一个共享科室，其他所有的科室都可以进行调用

![](./img/moe4.png)

关于效果，DeepSeekMoE 在同等计算代价的情况下，相比稠密网络和普通MoE的效果都有很大的提升，并与每次都全部激活的稠密网络的性能相当。

## MLA

DeepSeek V2 中，保留了 MoE 机制，同时对注意力进行了修改，提出multi-head latent attention多头潜在注意力机制。

我们之前聊过的 KV cache 减少了推理时的计算量，加快了推理速度，但是它是以宝贵的显存空间来换取计算量的减少的。并且随着生成序列越来越长，KV cache会越来越大。 对此，人们也想了很多办法来代替多头注意力（MHA）。包括减少 KV 数量的 multi-query attention（MQA）和 group-query attention（GQA），这两种方法可以减少显存大小，但是也显著影响了模型性能

那么有没有减少显存大小又不影响模型性能，甚至可以提高模型性能的做法呢？ 这种方法被DeepSeek 找到了，那就是MLA 多头潜在注意力机制。它的原理也很简单：

1. 首先对token的特征向量通过一个参数矩阵进行压缩， 我们把它叫做 $$W^{DKV}$$ ，其中D是down的意思是向下压缩，KV就是K向量和v向量的意思。 比如原来的特征维度为 6，经过压缩到 2 维，然后只需要缓存这个 2 维的 KV 压缩向量
2. 在进行计算需要用到真实的K和V向量时，通过两个减压矩阵（$$W^{UK},W^{UV}$$）转化为原来的维度就可以。

![](./img/mla1.png)

这时我们可以比较一下原始的MHA 、MPV以及MLA的缓存占用量。 MLA确实能减少缓存，而且实验表明，MLA的模型效果比MHA还要好，提升了性能。

这一切都非常不错。但是KV cache的本意是什么呢？它是为了减少推理时间。 MLA因为缓存了压缩的KV减少了缓存，但是在取出缓存后，K和V不能直接使用，还是要经过减压计算才可以，又引入了新的计算和时间，这和KV cache的初衷是相悖的。

![](./img/mla2.png)

对于标准的带KV cache的注意力计算，对于当前的token计算QKV，然后缓存K和V，对于之前的token直接从缓存里取出K和V向量就可以，然后计算。

对于MLA的计算，Q的计算不变，但在K和V的计算时，需要进行压缩生成 $$C^{KV}$$ 向量，存入缓存，如果要得到当前和之前 token 的 K 和 V，需要通过解压矩阵获得

有一个加速的方式是，在MLA的计算公式中，最后得到的 $$W^QW^{UK^T}$$ 可以提前计算好，这样就省去了解压计算所带来的额外时间

![](./img/mla3.png)

此外，Q的计算也可以引入同样的压缩和解压操作，这样减少了模型参数量并且也提升的效果，与 KV 不同的是 Q 的隐向量不需要缓存

但是当我们引入了旋转位置编码呢？下面是不带旋转位置编码的公式推导
$$
\begin{aligned}
q_{i} k_{j}^{T} &= h_{i}W^{Q}\left(c_{j}^{K V}W^{UK}\right)^{T} \\
&= h_{i}W^{Q}W^{UK^{T}}c_{j}^{KV^{T}} \\
&= h_{i}\textcolor{red}{W^{QUK}}c_{j}^{KV^{T}}
\end{aligned}
$$
对于RoPE，根据 token 位置的不同，旋转矩阵的参数也是不同的，如果增加了旋转矩阵 R ，它就出现在了 $$W^{Q}$$ 和 $$W^{UK^{T}}$$ 之间，而且因为 R 和位置相关，不能和这两个矩阵进行融合，所以破坏了之前想到的矩阵提前融合的方案。
$$
\begin{aligned}
q_{i} R_{i} (k_{j} R_{j})^{T} &= h_{i}W^{Q}R_{i} \left(c_{j}^{KV}W^{UK}R_{j}\right)^{T} \\
&= h_{i}W^{Q}R_{i}R_{j}^{T}W^{UK^{T}}c_{j}^{KV^{T}}
\end{aligned}
$$
DeepSeek最终想到了一个解决方案，就是给 Q 和 K 向量额外增加一些维度来表示位置信息

1. 对于Q向量，它通过 $$W^{QR}$$ 为每一个头生成一些原始特征。然后通过旋转位置编码增加位置信息。再把生成带位置信息的特征拼接到每个注意力头的Q向量
2. 对于K向量，通过 $$W^{KR}$$ 矩阵生成一个头共享的特征，然后通过旋转位置编码增加位置信息，然后复制到多个头共享位置信息。这里多头共享带位置编码的K向量也需要被缓存，以便在生成带位置信息的K向量时用到

![](./img/mla4.png)

最后，将不带旋转位置编码的部分和旋转位置编码相加即可
$$
q_{i} k_{j}^{T}+q_{i}^R k_{j}^{R^T}= h_{i}{W^{QUK}}c_{j}^{KV^{T}}+ q_{i}^R k_{j}^{R^T}
$$
最后看一下论文中的mla图：

![](./img/mla5.png)

1. 通过 h 生成压缩的KV特征和压缩的Q特征
2. 压缩的 KV 特征解压为多头的K和V特征
3. 从输入特征 h 生成多头共享的带旋转位置编码的 $$k^R$$
4. 再把 $$k^C$$ 和 $$k^R$$ 合并，形成最终带位置编码的K向量
5. 通过解压生成多头的Q向量，然后从压缩的Q向量生成多头带位置编码的 $$q^R$$。合并 $$q^C$$ 和 $$q^R$$ 生成最终在位置编码的Q向量
6. 接着QKV向量进行多头注意力计算

注意图中阴影部分为需要缓存的中间变量，其中只有KV共用的压缩隐特征以及K的多头共享的带位置编码的向量需要缓存。

$$k^R$$ 可以不共享吗？ $$q^R$$ 可以从 h 中计算得来吗？最好的方法肯定是

- 从 h 向量直接计算
- 不共享

但是这样会大大增加显存使用和降低计算效率。原理上，基于潜向量 c 计算RoPE肯定是有损的，共享也肯定牺牲了表达能力，所以做了一些权衡：

1. Q向量都基于潜向量 c 生成RoPE向量而不共享，主要是为了增加计算效率。因为潜向量 c 小所以计算更快，而且每次都要计算。不共享是为了保证表达能力。
2. K向量是从缓存中取的，不用每次计算，所以直接在H中计算就好。但是如果不共享将会让每个头都有一个RoPE向量，大大增加显存占用，所以共享。

## MTP

DeepSeek V3 在推出时，效果超越了 openai 的 4o 模型，更令人震惊的是，它的训练成本竟只有500多万美元。下表展示了预估开销，假设 H800 GPU 租赁价格为 $2 每小时

| Training Costs       | Pre-Training | Context Extension | Post-Training | Total    |
|----------------------|--------------|-------------------|---------------|---------|
| in H800 GPU Hours    | 2664K        | 119K              | 5K            | 2788K   |
| in USD               | 5.328M      | 0.238M           | 0.01M        | 5.576M |||

另一个对比是，Llama3-405B 在更高级的H100显卡上训练了3080w小时，而DeepSeek V3在H800 GPU上也只用了279w小时，不到Llama3的1/10，效果更好。这让 DeepSeek 火出了圈，也否定了之前无脑堆算力来提升模型性能的做法，也引起了英伟达股价的震动。

DeepSeek V3除了在模型架构方面做了一些优化和改动外。也在训练框架方面做了非常多的改进，例如它是业界首次使用FP8混合精度训练超大参数量模型。

DeepSeek V3 采用了我们之前讲过的 MoE 和 MLA，在实践细节上有一些小的变化。在DeepSeek V2里，路由网络出来的各个专家的权重 logits 用softmax作为激活函数，这样让所有专家的权重加和为一。但是因为最终只选择 topK 个专家，被选择的这 topK 专家对权重还要再做一次归一化，所以这里也不一定必须用softmax。在DeepSeek V3里，这里就改成对每个专家权重的 logits 用sigmoid的函数作为激活函数。然后对选择的 topK 个专家再进行归一化，作为最终的专家权重。

V2 的专家权重（公式从下往上看）：
$$
\begin{aligned}
\mathbf{h}_{t}^{\prime} &= \mathbf{u}_{t} + \sum_{i=1}^{N_{s}}\mathrm{FFN}_{i}^{(s)}\left(\mathbf{u}_{t}\right) + \sum_{i=1}^{N_{r}} g_{i,t}\mathrm{FFN}_{i}^{(r)}\left(\mathbf{u}_{t}\right), \\
g_{i, t} &= \left\{
\begin{array}{ll}
s_{i, t}, & s_{i,t} \in {\operatorname{Topk}}\left(\left\{s_{j, t} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right), \\
0, & \text{otherwise},
\end{array}
\right. \\
s_{i, t} &= \operatorname{Softmax}_{i}\left(\mathbf{u}_{t}^{T}\mathbf{e}_{i}\right),
\end{aligned}
$$
V3 的专家权重：
$$
\begin{aligned}
\mathbf{h}_{t}^{\prime} &= \mathbf{u}_{t} + \sum_{i=1}^{N_{s}}\mathrm{FFN}_{i}^{(s)}\left(\mathbf{u}_{t}\right) + \sum_{i=1}^{N_{r}} g_{i,t}\mathrm{FFN}_{i}^{(r)}\left(\mathbf{u}_{t}\right), \\
g_{i, t} &= \frac{g_{i, t}^{\prime}}{\sum_{j=1}^{N_{r}} g_{j,t}^{\prime}} \\
g_{i, t}^{\prime} &= \left\{
\begin{array}{ll}
s_{i, t}, & s_{i,t} \in \operatorname{Topk}\left(\left\{s_{j, t} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right), \\
0, & \text{otherwise},
\end{array}
\right. \\
s_{i, t} &= \operatorname{Sigmoid}\left(\mathbf{u}_{t}^{T}\mathbf{e}_{i}\right),
\end{aligned}
$$
接下来是负载均衡的修改，之前 deepseek MoE 讲过针对 batch 的辅助负载均衡损失函数。在DeepSeek V3里去掉了针对batch的辅助负载均衡损失函数，而是在训练时，在每个step，针对每个专家的负载进行监控：如果某个专家负载过高，就减小它对应的偏置值 $$b_i$$ , 如果专家的负载过低，就增加它的偏置值。
$$
g_{i,t}^{\prime} = 
\begin{cases}
s_{i,t}, & s_{i,t}+{b_{i}} \in \text{Topk}\left(\left\{s_{j,t}+b_{j} \mid 1\leqslant j\leqslant N_{r}\right\}, K_{r}\right), \\
0, & \text{otherwise.}
\end{cases}
$$
需要注意的是，这个偏置值只在专家路由时起作用，在和最终专家的输出相乘时，还是采用原来的权重值， 不会加这个偏置值。

另外增加了针对序列的负载均衡损失函数，这个损失函数的形式和DeepSeekMoE针对batch的负载均动函数损失是一致的，都是用各个专家被选择的频率值乘以平均概率值再求和。
$$
\begin{aligned}
\mathcal{L}_{\text{Bal}} &= \alpha \sum_{i=1}^{N_{r}} {f_{i}} {P}_{i}, \\
f_{i} &= \frac{N_{r}}{K_{r} T} \sum_{t=1}^{T} \mathbb{1}\left(s_{i, t} \in \operatorname{Topk}\left(\left\{s_{j, t} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right)\right), \\
s_{i, t}^{\prime} &= \frac{s_{i, t}}{\sum_{j=1}^{N_{r}} s_{j, t}}, \\
P_{i} &= \frac{1}{T} \sum_{t=1}^{T} s_{i, t}^{\prime},
\end{aligned}
$$
之所以要增加序列负载均衡损失函数，是防止在序列内部专家负载不均衡。如果专家仅仅是在 batch 内负载均衡，那有可能是按照序列划分的专家，有的专家更能擅长回答某一类问题。这样，当模型部署后，如果用户在某一时间问多个同一方向的问题，就会出现专家负载不均衡的现象。

DeepSeek也做了消融实验，发现去掉 batch 的辅助负载均衡损失函数，通过动态调节每个专家的权重偏置的方法可以获得获得更好的模型效果，并且在不同大小的模型效果都是一致的。

下一个大的改动就是训练时采用了多 token 预测（MTP）之前我们学的大语言模型都是每次只能预测一个 token，但实际上像Google、meta等公司的研究人员一直在研究让 LLM 一次预测多个 token。

下图是 DeepSeek V3 出现之前的 MTP 模型架构。最后一层的 transformer block 改为和多个 token 预测的头放在一起，它的作用并没有改变还是预测下一个 token，从 （L-1） 层接入token的特征。再加一个共享参数的 output head

![](./img/mtp1.png)

> 例如，输入是 t1，原始的输出头预测 t2，下一个MTP头需要预测 t3，再下一个MTP头则需要预测 t4。

在计算loss时，会分别考虑 t2、t3、t4 的交叉熵损失，其中 t2 的损失权重会大一些，t3 和t4 的损失权重会小一些，这样做是有一定道理的，那就是我们人在说一句话时，大脑并不是一个字一个字生成序列，很有可能一下子就思考出一个片段。

具体到模型训练时，对于骨干网的训练监督信号，由原来的下一个token变为多个token，这可以增加训练信号，提高数据的利用率，另外这样也迫使微信可以提前规划它提取的特征，更好的预测未来的token。多 token 的预测还有一个好处就是可以加速预测，它的加速预测不是简单的直接用多个头来生成接下来的多个 token，因为预测最准确的还是预测下一个token 的那个头，预测越远的 token 越不准确，需要进行修正。下面我们看一下利用MTP进行预测加速的方法。

假设输入“天生我”，MTP 同时预测出 “才必有人” 共四个 token，在这一步 "才" 最近也最准确，图示为深色。其余还不确信为浅色，那么对于不确信的token该怎么办呢？答案就是拿第一个头再去验证一下。

![](./img/mtp2.png)

所以我们构造了这样如上的 batch，用第一个头来预测下一个头，对不确信的token逐个进行确认。这里前面两个对了，第三个错了。需要注意的是，这里三个序列是通过一个 batch 并行进行验证的（因为GPU擅长并行计算），主要时间开销是显存访问。所以认为这一个 batch 3个序列的验证时间，和一个序列生成 1 个 token 的时间差不多。

最后我们接受最长的正确序列。虽然第四个头预测错了，但是验证时给出了更确信的答案，最终接受时就接受更确信的答案。可以看到，通过预测和验证两步运算就预测出来4个token，原来这需要四步运算才可以完成。所以 MTP 可以加速推理，但这是以增加GPU的计算量为代价的（第二步要同时对三个序列进行预测）

更进一步，可以将验证和预测放在一起。假如理想情况下四个头都预测正确，在验证时，我们之前是只让头去预测下一个 token，现在我们让头输出多个 token。比如验证时，以4个序列为一个batch，则在验证完成后就又可以进入下一步的验证了，这样不断地进行验证加预测的循环，就可以在每个 batch 结束后输出接受的序列，效率更高。

![](./img/mtp3.png)

MTP 预测加速是在生成单个序列时有效，就是整个集群只为你一个人做推理服务，那确实可以加速推理。但是我们知道现在像vllm这样的推理框架支持动态 batch，可以支持多用户同时访问 LLM，它会自动的将多个不同的请求组合成 batch 进行预测，这是 MTP 的预测加速就没有优势了，反而会因为可能的验证不通过而浪费计算。

所以 DeepSeek MTP 只是在训练时利用 MTP 来提升模型的性能，在推理部署时一般就丢弃掉其他几个预测多步的头，就只用第一个头来做下一个 token 的预测，和普通单头的大模型没有区别。

DeepSeek MTP 在模型架构上也进行了修改。原来的多头的预测有个问题就是法尔多部的口感不知道它前面的几个token，从而让预测正确率变低，不利于模型的修理。比如对于第四个的预测。他要在不知道他前面三个token的前提下预测出下一个token这样难度就非常大。就是给每个图传入了额外的信息，帮助多投给预测的头，能更好的预测出自己接下来的token。
