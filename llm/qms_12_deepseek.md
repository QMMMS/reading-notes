# DeepSeek 笔记

## MoE

MoE（mixture of experts）即混合专家模型，是一种降低训练和推理计算代价的技术。相对的，不使用 MoE 被称为 Dense 模型。

![](./img/moe1.png)

在预训练一个大模型时，我们设定期望损失和模型参数，对于相同的期望损失，如果我们加大模型参数，我们的训练计算花费会变少（不过推理花费会更大）。MoE 模型希望在达到同样期望效果时，训练计算和推理花费都变少。

![](./img/moe2.png)

MoE 主要对传统 decoder only 大模型中 feed forward 层进行了改造，原始的feed forward layer的内部结构很简单，例如先升维到二倍到四倍，在通过线性层变回原来大小

MoE 中，原来的 feed forward 层被拆分为多个小的 feed forward 层，这时每个小的 feed forward layer的第一层相比 Dense 结构的第一层的维度可以大幅减小。例如，从原来的4096乘16384减小为4096乘4096，第二层也一样，以保证输出token的维度还是 4096。 这里被一个小的feed forward layer就被称为一个专家expert。

那对于每个输入的token，应该选择哪个的专家网络呢？这需要一个路由网络来决定。路由网络输出每个token走不同专家网络的概率值，然后给出其中排名靠前的几个（例如两个）专家。token 通过选择的这两个专家网络，得到两个维度为4096的向量，然后再根据路由网络输出的专家权重进行加权求和，得到输出。

![](./img/moe3.png)

> 这里的专家选择是对每个 token 得出的，而不是序列

MoE 的特点是包括

- 在相同的计算代价下，增大了网络参数规模， 从而得到性能更好的大模型
- MOE网络基本可以达到和自己相同参数规模的稠密网络的性能表现
- 相比同等参数规模的稠密网络，MOE网络的计算代价变小了。但是所有专家网络参数都是要加载在显存中的，所以显存占用并没有减少

MoE 网络在训练时比稠密网络要难，因为会有专家在不均衡的问题：大量的token都被少数的几个专家处理，而其他的专家占用了网络参数，但是却不被激活。为了让MOE模型里的专家负载均衡，人们也想了很多办法。包括

- 训练时，每个token至少选择两个专家。如果每次只选择最好的一个专家，只有这个专家可以得到训练，它效果越来越好，权重越来越大，其他专家都得不到训练。所以一般训练时会选择权重最高的专家，同时以其他专家路由的权重作为概率，再随机选择一个专家
- 为了防止某些专家被过度训练，可以给每个专家设置token容量，也就是这个专家在一个batch里能处理的最多的token数，如果超过了这个token数，则对分配给这个专家的token输出为0， 这样这个token就通过残差连接进入下一层。
- 设置一个负载均衡的辅助损失，让模型在训练过程中自己学会负载均衡

我们来详细看一下负载均衡损失，它的目标是希望每个专家被调用的频率是相等的。它的计算公式如下：
$$
f_i = (该专家被调用的次数)/(所有专家给调用的次数)
$$

$$
\mathcal{L}_{\text{balance}}=\sum_i^n(f_i)^2
$$

这里可以通过柯西不等式来严格证明为什么只有负载均衡时，这个loss取最小值。

然而这个损失函数却不能直接拿来利用，因为我们计算每个专家被调用的频率，需要统计每个专家被调用次数，而每个专家是否被调用是通过topk操作进行的。topk操作是根据权重进行的一个选择操作，不是一个数值运算，它是不可微的，无法通过梯度下降进行优化。

一个近似的做法就是把频率的平方的一个频率 $$f_i$$ 用 $$p_i$$ 来代替，它是一个批次中所有token对该专家的路由的概率的平均值。理论上对该专家的路由的平均概率应该等于选择该专家的频率，同时路由的平均概率是通过softmax进行数据计算得到的结果，它是可微的。
$$
\mathcal{L}_{\text{balance}}=\sum_i^n f_i p_i
$$

> 这里虽然还保留了一个 $$f_i$$ ，不过整个式子已经可以被梯度下降优化，因为我们已经为反向传播给出了一个可微的通路，相当于把  $$f_i$$  作为常数

## DeepSeekMoE

DeepSeekMoE 的动机是让专家更加专精。基础的MOE实现中，假设有N个专家，每次选择两个专家，DeepSeek认为传统的MoE设置的专家数太少了，导致每个专家学习了过多的彼此不相关的知识，从而不够专精。

> 比如一个医院里只有两名医生，一个负责内科，一个负责外科，那每个医生需要掌握的医学领域就太广泛了，不利于他医术的进步。

DeepSeek 想到的办法就是将专家进行更进一步的细分，同时每个专家的网络也变小。比如将原来的N个专家变为2N个专家，每个专家网络参数量为原来专家的一半，这样在网络前向传播时，在和之前网络同样的计算代价的前提下，就可以激活四个专家。

> 原来是从8个专家里面选2个，有28种可能的组合；现在是从16个专家里面选4个，有1820种。可以看到对专家进行细分，可以得到更灵活的专家组合。

DeepSeek 又进一步想到，所有的专家可能都要学习一些基础的通用的能力，是否可以将所有专家都要学习的通用技术能力提取出来，作为一个共享专家，保证每次都会激活，他负责所有专家原来都需要的通用能力。然后再从后面(2N-1)专家里面选择3个专家，这样保证专家的总数不变。

> 这里可以理解为医院里面各个科室都需要验血，那就把验血作为一个共享科室，其他所有的科室都可以进行调用

![](./img/moe4.png)

关于效果，DeepSeekMoE 在同等计算代价的情况下，相比稠密网络和普通MoE的效果都有很大的提升，并与每次都全部激活的稠密网络的性能相当。

## MLA

DeepSeek V2 中，保留了 MoE 机制，同时对注意力进行了修改，提出multi-head latent attention多头潜在注意力机制。

我们之前聊过的 KV cache 减少了推理时的计算量，加快了推理速度，但是它是以宝贵的显存空间来换取计算量的减少的。并且随着生成序列越来越长，KV cache会越来越大。 对此，人们也想了很多办法来代替多头注意力（MHA）。包括减少 KV 数量的 multi-query attention（MQA）和 group-query attention（GQA），这两种方法可以减少显存大小，但是也显著影响了模型性能

那么有没有减少显存大小又不影响模型性能，甚至可以提高模型性能的做法呢？ 这种方法被DeepSeek 找到了，那就是MLA 多头潜在注意力机制。它的原理也很简单：

1. 首先对token的特征向量通过一个参数矩阵进行压缩， 我们把它叫做 $$W^{DKV}$$ ，其中D是down的意思是向下压缩，KV就是K向量和v向量的意思。 比如原来的特征维度为 6，经过压缩到 2 维，然后只需要缓存这个 2 维的 KV 压缩向量
2. 在进行计算需要用到真实的K和V向量时，通过两个减压矩阵（$$W^{UK},W^{UV}$$）转化为原来的维度就可以。

![](./img/mla1.png)

这时我们可以比较一下原始的MHAKV cache缓存量以及MPV的缓存量以及MLA的缓存占用量。 MLA确实能减少但是会影响模型效果吗？

