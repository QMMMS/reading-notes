# 其他方法

## Entropy Minimization

由于半监督学习的样本过少，所以一种比较好的思路就是将网络在上一次产生的，自信程度比较高的预测（我们就当他是预测对了的）加入到本次的训练中。因此，在训练的时候我们需要让网络的预测更加自信，而不是比较犹豫，即熵最小化 Entropy Minimization

> 也可以看作阻止网络的决策边界在数据点附近

对于分类问题，网络最后必然是通过一个softmax来输出，而softmax是所有的类别经过归一化之后自信程度。假设网络对每一个类的自信程度都相近，那么整体的熵值就会越大，反之则越小。这个方法我们可以通过增加一个对于预测的损失项来实现（信息论熵的定义）：
$$
-\sum_{k = 1}^{C}f_{\theta}(x)_k \log f_{\theta}(x)_k
$$
