#  Self-Supervision for SSL

自监督学习是无监督学习的一种形式，在这种学习中，模型使用标准的监督损失进行训练，但训练的是一个预训练任务，其监督信息来自数据本身。在这种情况下，目标不是最大化最终任务的性能，而是学习更具可迁移性的特征，以便于下游任务。

已经提出了各种预训练任务，其中模型首先在一个或多个带有无标签示例的任务上进行训练，得到的模型要么用于为原始数据生成微调，原始数据被用于在 $$D_l$$ 上训练浅层分类器，要么直接用于下游任务中的有标签图像。

此类计算机视觉预训练任务的示例有： 

- Exemplar-CNN：对于给定图像，使用不同的变换生成一组 $$N$$ 个图像块。然后将所有这些图像块视为不同的类别，并且训练模型以预测给定输入图像块的正确类别
- Rotation：在多个 $$90^{\circ}$$ 的四种可能旋转（即 $$[0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}]$$）中给定一个旋转应用于图像块，并且训练模型以预测应用的正确旋转。 
- Patches：首先从输入图像中随机提取一个图像块，将这个图像块视为中心，在八个相邻位置以小的抖动提取八个不同的相邻且不重叠的图像块，然后训练模型以预测第二个图像块相对于第一个图像块的位置。还提出了此预训练任务的其他版本，例如拼图游戏，其中九个图像块被打乱，模型的目标是预测为了得到图像块的正确顺序而应用的正确排列。 
- Colorization：首先将输入图像从 RGB 转换为 Lab 颜色空间，输入一个仅包含亮度信息（即 Lab 颜色空间中的 $$L$$ 分量）的图像到模型中，目标是预测图像的其余信息，即图像的亮度或颜色。该任务可以被视为回归问题或分类问题，通过量化 Lab 颜色空间来实现。 
- Contrastive Predictive Coding：使用基于噪声对比估计及其最新版本（如动量对比和 SimCLR）的对比损失，模型在正样本和负样本上进行训练和区分。正样本可以是给定的输入图像及其变换版本、给定的图像块及其相邻图像块，而负样本是随机采样的图像或图像块。 

这样的预训练任务可以很容易地用于 SSL，在半监督学习中，模型在整个数据集上针对预训练任务进行自监督训练，然后使用标准交叉熵损失在标记集 $$D_l$$ 上进行自适应调整。或者通过首先使用自监督训练模型，然后在 $$D_l$$ 上对其进行微调，可以实现半监督学习。 